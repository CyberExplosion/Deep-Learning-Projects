Use saved processed data...
load using file: P5/saved/cora-loadedData.pk
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_36b5d0: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 100]               100
│    │    └─Linear: 3-1                  [2708, 100]               143,300
│    │    └─SumAggregation: 3-2          [2708, 100]               --
│    └─ReLU: 2-2                         [2708, 100]               --
│    └─GCNConv: 2-3                      [2708, 50]                50
│    │    └─Linear: 3-3                  [2708, 50]                5,000
│    │    └─SumAggregation: 3-4          [2708, 50]                --
│    └─ReLU: 2-4                         [2708, 50]                --
│    └─GCNConv: 2-5                      [2708, 7]                 7
│    │    └─Linear: 3-5                  [2708, 7]                 350
│    │    └─SumAggregation: 3-6          [2708, 7]                 --
│    └─ReLU: 2-6                         [2708, 7]                 --
==========================================================================================
Total params: 148,807
Trainable params: 148,807
Non-trainable params: 0
Total mult-adds (M): 402.54
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 3.40
Params size (MB): 0.59
Estimated Total Size (MB): 19.61
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.10561299852289513 Loss: 1.9536445140838623
Epoch: 100 Accuracy: 0.47525849335302806 Loss: 1.526929259300232
Epoch: 200 Accuracy: 0.507754800590842 Loss: 1.0744946002960205
Epoch: 300 Accuracy: 0.5158788774002954 Loss: 0.9280931949615479
Epoch: 400 Accuracy: 0.5151403249630724 Loss: 0.8829693794250488
Epoch: 500 Accuracy: 0.5173559822747416 Loss: 0.8534078001976013
Epoch: 600 Accuracy: 0.5132939438700148 Loss: 0.8435090780258179
Epoch: 700 Accuracy: 0.5092319054652881 Loss: 0.8390254378318787
Epoch: 800 Accuracy: 0.5096011816838996 Loss: 0.8363314270973206
Epoch: 900 Accuracy: 0.5092319054652881 Loss: 0.8346577286720276
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.49      0.21      0.29       101
           1       0.00      0.00      0.00       130
           2       0.50      0.70      0.58       134
           3       0.75      0.02      0.05       127
           4       0.53      0.77      0.63       125
           5       0.47      0.79      0.59       124
           6       0.43      0.88      0.57       112

    accuracy                           0.48       853
   macro avg       0.45      0.48      0.39       853
weighted avg       0.45      0.48      0.39       853

saving models...
Accurarcy is: 48.06565064478312%
Use saved processed data...
load using file: P5/saved/cora-loadedData.pk
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_cc33ac: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 100]               100
│    │    └─Linear: 3-1                  [2708, 100]               143,300
│    │    └─SumAggregation: 3-2          [2708, 100]               --
│    └─ReLU: 2-2                         [2708, 100]               --
│    └─GCNConv: 2-3                      [2708, 50]                50
│    │    └─Linear: 3-3                  [2708, 50]                5,000
│    │    └─SumAggregation: 3-4          [2708, 50]                --
│    └─ReLU: 2-4                         [2708, 50]                --
│    └─GCNConv: 2-5                      [2708, 16]                16
│    │    └─Linear: 3-5                  [2708, 16]                800
│    │    └─SumAggregation: 3-6          [2708, 16]                --
│    └─ReLU: 2-6                         [2708, 16]                --
│    └─Linear: 2-7                       [2708, 7]                 119
│    └─ReLU: 2-8                         [2708, 7]                 --
==========================================================================================
Total params: 149,385
Trainable params: 149,385
Non-trainable params: 0
Total mult-adds (M): 404.09
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 3.75
Params size (MB): 0.60
Estimated Total Size (MB): 19.95
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.15472673559822747 Loss: 1.9455736875534058
Epoch: 100 Accuracy: 0.36115214180206795 Loss: 1.7477303743362427
Epoch: 200 Accuracy: 0.40620384047267355 Loss: 1.2886461019515991
Epoch: 300 Accuracy: 0.5461595273264401 Loss: 0.9295524954795837
Epoch: 400 Accuracy: 0.6074593796159528 Loss: 0.6666560769081116
Epoch: 500 Accuracy: 0.6096750369276218 Loss: 0.6070331931114197
Epoch: 600 Accuracy: 0.6100443131462334 Loss: 0.5851160287857056
Epoch: 700 Accuracy: 0.6122599704579025 Loss: 0.574491560459137
Epoch: 800 Accuracy: 0.6144756277695717 Loss: 0.5688035488128662
Epoch: 900 Accuracy: 0.6137370753323486 Loss: 0.5654328465461731
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.68      0.15      0.24       101
           1       0.49      0.69      0.58       130
           2       0.62      0.63      0.62       134
           3       0.56      0.81      0.66       127
           4       0.70      0.82      0.75       125
           5       0.00      0.00      0.00       124
           6       0.53      0.86      0.66       112

    accuracy                           0.58       853
   macro avg       0.51      0.57      0.50       853
weighted avg       0.51      0.58      0.51       853

saving models...
Accurarcy is: 57.561547479484176%
Use saved processed data...
load using file: P5/saved/cora-loadedData.pk
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_bbcc30: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 100]               100
│    │    └─Linear: 3-1                  [2708, 100]               143,300
│    │    └─SumAggregation: 3-2          [2708, 100]               --
│    └─ReLU: 2-2                         [2708, 100]               --
│    └─GCNConv: 2-3                      [2708, 50]                50
│    │    └─Linear: 3-3                  [2708, 50]                5,000
│    │    └─SumAggregation: 3-4          [2708, 50]                --
│    └─ReLU: 2-4                         [2708, 50]                --
│    └─GCNConv: 2-5                      [2708, 16]                16
│    │    └─Linear: 3-5                  [2708, 16]                800
│    │    └─SumAggregation: 3-6          [2708, 16]                --
│    └─ReLU: 2-6                         [2708, 16]                --
│    └─Linear: 2-7                       [2708, 7]                 119
│    └─ReLU: 2-8                         [2708, 7]                 --
==========================================================================================
Total params: 149,385
Trainable params: 149,385
Non-trainable params: 0
Total mult-adds (M): 404.09
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 3.75
Params size (MB): 0.60
Estimated Total Size (MB): 19.95
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.301698670605613 Loss: 1.9541072845458984
Epoch: 100 Accuracy: 0.4010339734121123 Loss: 1.7604199647903442
Epoch: 200 Accuracy: 0.40620384047267355 Loss: 1.2971653938293457
Epoch: 300 Accuracy: 0.41986706056129985 Loss: 1.0038237571716309
Epoch: 400 Accuracy: 0.42134416543574593 Loss: 0.9151358604431152
Epoch: 500 Accuracy: 0.4202363367799114 Loss: 0.8827161192893982
Epoch: 600 Accuracy: 0.4194977843426883 Loss: 0.8655185699462891
Epoch: 700 Accuracy: 0.4242983751846381 Loss: 0.8548876047134399
Epoch: 800 Accuracy: 0.42319054652880356 Loss: 0.8483809232711792
Epoch: 900 Accuracy: 0.422821270310192 Loss: 0.8440629839897156
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.41      0.17      0.24       150
           1       0.40      0.71      0.51       150
           2       0.40      0.79      0.53       150
           3       0.46      0.61      0.53       150
           4       0.00      0.00      0.00       150
           5       0.00      0.00      0.00       150
           6       0.54      0.81      0.65       150

    accuracy                           0.44      1050
   macro avg       0.32      0.44      0.35      1050
weighted avg       0.32      0.44      0.35      1050

saving models...
Accurarcy is: 44.19047619047619%
Use saved processed data...
load using file: P5/saved/cora-loadedData.pk
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_86318c: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 100]               100
│    │    └─Linear: 3-1                  [2708, 100]               143,300
│    │    └─SumAggregation: 3-2          [2708, 100]               --
│    └─ReLU: 2-2                         [2708, 100]               --
│    └─GCNConv: 2-3                      [2708, 50]                50
│    │    └─Linear: 3-3                  [2708, 50]                5,000
│    │    └─SumAggregation: 3-4          [2708, 50]                --
│    └─ReLU: 2-4                         [2708, 50]                --
│    └─GCNConv: 2-5                      [2708, 7]                 7
│    │    └─Linear: 3-5                  [2708, 7]                 350
│    │    └─SumAggregation: 3-6          [2708, 7]                 --
│    └─Sigmoid: 2-6                      [2708, 7]                 --
==========================================================================================
Total params: 148,807
Trainable params: 148,807
Non-trainable params: 0
Total mult-adds (M): 402.54
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 3.40
Params size (MB): 0.59
Estimated Total Size (MB): 19.61
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.14918759231905465 Loss: 1.9478851556777954
Epoch: 100 Accuracy: 0.6070901033973413 Loss: 1.7879881858825684
Epoch: 200 Accuracy: 0.6794682422451994 Loss: 1.539207935333252
Epoch: 300 Accuracy: 0.7016248153618907 Loss: 1.3549777269363403
Epoch: 400 Accuracy: 0.7008862629246676 Loss: 1.2688649892807007
Epoch: 500 Accuracy: 0.69903988183161 Loss: 1.2295678853988647
Epoch: 600 Accuracy: 0.7042097488921714 Loss: 1.2091649770736694
Epoch: 700 Accuracy: 0.7019940915805022 Loss: 1.1972143650054932
Epoch: 800 Accuracy: 0.6986706056129985 Loss: 1.1896425485610962
Epoch: 900 Accuracy: 0.6964549483013294 Loss: 1.1845091581344604
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.69      0.67      0.68       150
           1       0.66      0.67      0.67       150
           2       0.63      0.75      0.68       150
           3       0.66      0.52      0.58       150
           4       0.77      0.89      0.82       150
           5       0.60      0.63      0.61       150
           6       0.90      0.73      0.81       150

    accuracy                           0.70      1050
   macro avg       0.70      0.70      0.69      1050
weighted avg       0.70      0.70      0.69      1050

saving models...
Accurarcy is: 69.52380952380952%
Loading cora dataset...
Unique labels [0 1 2 3 4 5 6]
The index train: [1420, 2415, 2338, 66, 785, 2122, 922, 1033, 49, 1535, 937, 1674, 1597, 1401, 580, 583, 1350, 1002, 603, 1278, 845, 2125, 444, 1300, 268, 338, 1488, 2491, 1891, 1923, 2230, 351, 617, 1289, 2374, 169, 462, 2584, 175, 2571, 82, 738, 2444, 1741, 490, 1954, 210, 321, 2209, 370, 564, 2309, 466, 568, 879, 750, 473, 567, 333, 239, 1543, 2234, 889, 452, 855, 1901, 2561, 161, 2506, 532, 1582, 852, 339, 2127, 1029, 2525, 1393, 1764, 1437, 515, 1164, 618, 430, 2329, 1444, 848, 2054, 1897, 2172, 130, 2331, 2583, 2036, 1306, 251, 1006, 1433, 1105, 2180, 1026, 744, 2705, 1807, 1136, 1369, 1127, 1034, 2261, 813, 1959, 1396, 1333, 1481, 2130, 658, 1134, 1207, 1231, 1930, 191, 2539, 426, 2636, 1720, 1721, 735, 877, 2649, 1442, 2534, 756, 1297, 348, 1071, 2100, 2591, 2587, 2171, 1049, 1651] and index test: [1407, 991, 2276, 470, 445, 1498, 2568, 1089, 2057, 2556, 166, 2593, 288, 231, 457, 61, 1047, 1665, 1040, 901, 760, 818, 1107, 783, 1155, 1695, 1855, 2616, 1626, 1554, 1344, 1621, 824, 2289, 2585, 690, 2061, 236, 1854, 582, 50, 1584, 1531, 1348, 266, 222, 2319, 578, 1864, 2605, 1148, 2128, 2466, 1707, 1416, 2048, 799, 634, 2665, 2581, 1537, 183, 2520, 801, 862, 1209, 686, 149, 69, 1413, 2038, 649, 2035, 313, 150, 1111, 2215, 2471, 643, 983, 670, 284, 1617, 1354, 2037, 1048, 172, 1556, 928, 1475, 2635, 831, 2245, 1317, 1805, 1126, 1263, 1377, 1804, 543, 1699, 1574, 1568, 1233, 2303, 318, 1138, 1653, 2403, 1752, 1046, 1483, 1101, 2544, 2473, 2244, 1552, 32, 1084, 1553, 1570, 1121, 2052, 2371, 1152, 752, 262, 819, 2157, 1001, 2614, 1904, 2299, 1987, 1882, 629, 1680, 825, 1860, 96, 300, 259, 1304, 2656, 2308, 1497, 534, 927, 2071, 2395, 341, 1997, 873, 906, 1112, 1822, 1697, 2307, 808, 1055, 1103, 1763, 979, 1387, 1043, 393, 516, 1526, 1866, 874, 2239, 2681, 263, 2284, 1871, 406, 90, 146, 1423, 1472, 589, 2147, 1493, 1196, 1760, 2325, 1157, 962, 7, 364, 2535, 2542, 217, 2043, 2608, 192, 2413, 383, 1821, 2701, 625, 1812, 1341, 1365, 320, 2386, 1394, 1928, 1014, 2281, 2642, 708, 301, 1356, 1005, 2155, 572, 1247, 702, 1146, 1579, 2457, 1916, 912, 112, 2552, 814, 1295, 2439, 2570, 1088, 1323, 269, 1464, 526, 2526, 1778, 666, 620, 2632, 1242, 809, 2513, 2352, 2049, 2576, 2680, 28, 1176, 1272, 365, 198, 2448, 918, 2154, 2119, 2185, 1283, 1929, 2358, 1667, 784, 1431, 1021, 2670, 810, 622, 474, 2567, 1229, 864, 1915, 2304, 1340, 257, 427, 705, 384, 753, 1591, 1374, 2382, 2400, 234, 2191, 404, 1093, 2683, 125, 719, 407, 1471, 1787, 840, 405, 1766, 1801, 1948, 2557, 949, 197, 1585, 1859, 2368, 132, 1540, 34, 1438, 1440, 551, 298, 63, 1754, 186, 21, 875, 2628, 2174, 500, 2629, 842, 1779, 691, 988, 1979, 688, 739, 559, 1228, 1957, 167, 463, 148, 446, 1844, 367, 2305, 734, 39, 1557, 2014, 653, 24, 2027, 60, 807, 2586, 128, 548, 385, 1529, 3, 472, 1573, 281, 565, 1975, 162, 252, 1213, 1217, 2222, 588, 410, 1137, 73, 633, 1711, 294, 506, 85, 2494, 2183, 1515, 868, 699, 2150, 1193, 1447, 1693, 1192, 2666, 1589, 2485, 455, 554, 471, 1183, 2406, 502, 2207, 492, 1376, 682, 35, 425, 1253, 671, 2007, 644, 733, 274, 2453, 601, 562, 45, 1581, 16, 773, 2411, 37, 1316, 1378, 1649, 1816, 882, 2408, 566, 2328, 103, 118, 220, 2551, 1791, 247, 477, 1202, 1546, 2432, 552, 1758, 775, 1782, 636, 1045, 1599, 804, 621, 575, 412, 933, 1703, 2179, 1625, 2118, 2655, 1767, 650, 2412, 1643, 2420, 1802, 1414, 2661, 894, 2404, 1291, 952, 278, 711, 2064, 2362, 892, 2428, 275, 1977, 637, 2492, 1502, 419, 787, 1716, 2510, 51, 2620, 1978, 2318, 772, 95, 742, 117, 1326, 325, 218, 1942, 495, 1288, 4, 1503, 1372, 1909, 99, 1133, 2419, 1982, 2682, 107, 2698, 1128, 137, 1064, 1425, 2161, 985, 557, 1427, 513, 1991, 2460, 1308, 2596, 2700, 1504, 1749, 2159, 954, 484, 327, 182, 851, 1076, 1078, 1415, 777, 2443, 1549, 1761, 2468, 2169, 1381, 1602, 1731, 2034, 326, 1506, 2114, 2699, 1898, 67, 1375, 2458, 1934, 895, 2669, 1995, 802, 1162, 915, 2148, 1548, 1436, 108, 1993, 2136, 632, 498, 1403, 1172, 1742, 1714, 450, 723, 558, 2313, 2693, 2311, 2559, 2677, 2025, 1241, 2287, 491, 5, 976, 1512, 1913, 2081, 897, 115, 170, 358, 987, 1276, 2097, 646, 1593, 1525, 2592, 2461, 324, 1013, 1759, 641, 619, 2266, 944, 1900, 185, 584, 2654, 2690, 208, 2481, 264, 2099, 2073, 1669, 481, 740, 2324, 2555, 2367, 1360, 2315, 525, 2028, 1170, 114, 1762, 1310, 1343, 1053, 1521, 299, 1586, 1132, 1692, 2040, 1364, 1025, 1174, 2197, 507, 2696, 859, 1510, 52, 1712, 762, 1237, 2110, 569, 2320, 2423, 2006, 847, 79, 1273, 2657, 2651, 1924, 1964, 767, 1853, 614, 1227, 1848, 518, 109, 555, 438, 1967, 2467, 635, 171, 1027, 2113, 2508, 2511, 2031, 1863, 571, 876, 2528, 790, 89, 2146, 2134, 1683, 1337, 2360, 514, 2145, 1918, 1566, 2633, 1678, 2483, 2495, 1181, 418, 436, 1456, 1095, 1484, 151, 1191, 267, 535, 2675, 1576, 556, 706, 1400, 861, 794, 1491, 1032, 1505, 1236, 1074, 2168, 287, 2644, 2695, 791, 2346, 1062, 1000, 640, 1361, 2623, 932, 1449, 431, 2326, 2509, 977, 2356, 317, 319, 1561, 2011, 957, 375, 2694, 1875, 1702, 2092, 2066, 1708, 2273, 242, 1827, 1363, 2548, 1792, 1727, 865, 2627, 1404, 1467, 1673, 2058, 1872, 2630, 188, 2637, 2259, 815, 961, 237, 1921, 1391, 537, 2490, 2316, 1035, 168, 2087, 1999, 1311, 2702, 1796, 1362, 1036, 2704, 2659, 1980, 243, 1439, 1594, 2039, 2317, 721, 1473, 2347, 402, 240, 982, 523, 1009, 1694, 2211, 2298, 811, 1840, 2330, 728, 100, 1685, 309, 935, 2265, 1077, 1098, 1392, 546, 1831, 22, 11, 448, 2521, 1740, 758, 1432, 1533, 2641, 2232, 2139, 329, 953, 1516, 1890, 380, 2220, 2541, 1756, 1159, 1099, 1249, 1836, 2504, 1232, 856, 1971, 1395, 530, 1611, 1717, 689, 1225, 2196, 940, 2446, 145, 1518, 2199, 415, 2065, 934, 1299, 1114, 1409, 2003, 2598, 228, 2563, 1140, 2205, 1790, 1523, 1380, 2516, 755, 1984, 757, 1379, 1167, 681, 1106, 1353, 1730, 189, 422, 972, 1216, 42, 959, 1925, 2353, 1646, 2226, 1628, 1424, 964, 1143, 880, 717, 1261, 2267, 1113, 1738, 938, 1565, 1277, 1880, 1474, 1826, 1351, 893, 837, 2312, 2096, 432, 2070, 2527, 119, 1489, 2229, 1301, 2009, 652, 2079, 1428, 1008, 396, 2424, 2282, 1849, 1271, 1208, 2126, 1244, 1620, 1102, 1622, 1275, 1955, 997, 135, 2550, 1165, 675, 1226, 1519, 1578, 1151, 2339, 1837, 597, 1312, 2106, 521, 2045, 335, 314, 1596, 1256, 1180, 1750, 1327, 2450, 1177, 1684, 1234, 249, 960, 428, 184, 2493, 1292, 2290, 963, 1819, 2190, 433, 156, 2158, 598, 1085, 2132, 1445, 2213, 286, 2589, 2240, 1081, 1539, 1587, 87, 1082, 1961, 106, 1, 1182, 196, 1010, 1679, 136, 2449, 1609, 316, 1726, 1686, 2291, 379, 2194, 307, 224, 2663, 1073, 86, 1007, 1569, 1061, 509, 654, 2188, 669, 1429, 2640, 849, 344, 2163, 2668, 2650, 373, 1486, 2441, 1434, 1370, 2142, 1627, 707, 2149, 2032, 2189, 2365, 223, 176, 585, 1919, 2431, 1956, 2253, 1944, 1776, 2609, 2426, 302, 1781, 2602, 1259, 1011, 2300, 2217, 216, 195, 158]
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_fe5b7d: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 100]               100
│    │    └─Linear: 3-1                  [2708, 100]               143,300
│    │    └─SumAggregation: 3-2          [2708, 100]               --
│    └─ReLU: 2-2                         [2708, 100]               --
│    └─GCNConv: 2-3                      [2708, 50]                50
│    │    └─Linear: 3-3                  [2708, 50]                5,000
│    │    └─SumAggregation: 3-4          [2708, 50]                --
│    └─ReLU: 2-4                         [2708, 50]                --
│    └─GCNConv: 2-5                      [2708, 7]                 7
│    │    └─Linear: 3-5                  [2708, 7]                 350
│    │    └─SumAggregation: 3-6          [2708, 7]                 --
│    └─Sigmoid: 2-6                      [2708, 7]                 --
==========================================================================================
Total params: 148,807
Trainable params: 148,807
Non-trainable params: 0
Total mult-adds (M): 402.54
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 3.40
Params size (MB): 0.59
Estimated Total Size (MB): 19.61
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.11447562776957164 Loss: 1.9438378810882568
Epoch: 100 Accuracy: 0.6292466765140325 Loss: 1.7618032693862915
Epoch: 200 Accuracy: 0.7149187592319055 Loss: 1.4969593286514282
Epoch: 300 Accuracy: 0.7319054652880355 Loss: 1.3258304595947266
Epoch: 400 Accuracy: 0.7374446085672083 Loss: 1.252363920211792
Epoch: 500 Accuracy: 0.7348596750369276 Loss: 1.2194684743881226
Epoch: 600 Accuracy: 0.7300590841949779 Loss: 1.202458381652832
Epoch: 700 Accuracy: 0.7245199409158051 Loss: 1.192562222480774
Epoch: 800 Accuracy: 0.7211964549483013 Loss: 1.186348795890808
Epoch: 900 Accuracy: 0.7197193500738552 Loss: 1.182200312614441
Epoch: 1000 Accuracy: 0.7175036927621861 Loss: 1.179239273071289
Epoch: 1100 Accuracy: 0.71602658788774 Loss: 1.1768298149108887
Epoch: 1200 Accuracy: 0.7130723781388478 Loss: 1.174646258354187
Epoch: 1300 Accuracy: 0.7127031019202363 Loss: 1.1729882955551147
Epoch: 1400 Accuracy: 0.7127031019202363 Loss: 1.1717619895935059
Epoch: 1500 Accuracy: 0.7123338257016248 Loss: 1.1708353757858276
Epoch: 1600 Accuracy: 0.7115952732644018 Loss: 1.1700676679611206
Epoch: 1700 Accuracy: 0.7115952732644018 Loss: 1.1692700386047363
Epoch: 1800 Accuracy: 0.7112259970457903 Loss: 1.1685864925384521
Epoch: 1900 Accuracy: 0.7108567208271788 Loss: 1.1680535078048706
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.64      0.60      0.62       150
           1       0.58      0.61      0.60       150
           2       0.82      0.81      0.81       150
           3       0.88      0.67      0.76       150
           4       0.56      0.67      0.61       150
           5       0.67      0.89      0.76       150
           6       0.83      0.63      0.72       150

    accuracy                           0.70      1050
   macro avg       0.71      0.70      0.70      1050
weighted avg       0.71      0.70      0.70      1050

saving models...
Accurarcy is: 69.61904761904762%
Loading cora dataset...
Unique labels [0 1 2 3 4 5 6]
The index train: [249, 654, 2312, 1271, 2217, 196, 316, 1165, 1781, 1919, 1519, 2436, 1177, 2194, 707, 216, 893, 2032, 1569, 1008, 936, 24, 671, 466, 559, 492, 562, 1741, 1687, 460, 45, 1193, 750, 552, 1844, 2118, 333, 82, 2472, 2411, 1574, 603, 1674, 1904, 643, 434, 2244, 1121, 1304, 1496, 683, 835, 799, 236, 486, 2466, 2215, 1854, 2607, 221, 1663, 2216, 959, 1409, 1836, 1225, 2446, 2504, 1249, 188, 1175, 1696, 2086, 2705, 1065, 1839, 2165, 854, 1541, 1999, 1676, 1724, 1246, 1615, 1382, 1886, 154, 1365, 1998, 1746, 2683, 291, 2154, 1822, 1485, 2374, 611, 366, 2135, 1470, 1128, 945, 1608, 484, 2620, 1660, 1357, 1783, 2693, 2468, 1624, 1335, 324, 1742, 2323, 2088, 270, 138, 2543, 155, 924, 822, 2326, 1618, 2031, 2618, 2028, 1166, 267, 955, 638, 1492, 722, 1827, 2685, 400, 791, 1388, 767, 1607] and index test: [1578, 1955, 2195, 396, 158, 2609, 1061, 224, 2253, 2617, 1721, 1007, 2189, 2045, 1102, 379, 1881, 1085, 1151, 417, 178, 652, 2009, 1609, 2449, 2126, 1445, 585, 2589, 1837, 2602, 2424, 86, 2649, 735, 1370, 2300, 1301, 2240, 344, 2365, 1587, 1750, 1256, 509, 2527, 307, 1651, 1244, 2229, 176, 135, 348, 1776, 106, 426, 1620, 2079, 1539, 165, 1819, 2132, 1, 2158, 1180, 1082, 675, 1963, 373, 598, 2171, 2188, 335, 314, 2213, 1297, 1275, 1327, 597, 1720, 1428, 2070, 2640, 2539, 756, 1528, 2163, 2587, 1071, 1226, 428, 669, 2149, 2190, 1073, 2663, 1944, 1961, 1679, 963, 119, 2450, 87, 2550, 849, 433, 1292, 1259, 432, 136, 1726, 1486, 1312, 2431, 2493, 1442, 156, 184, 1049, 2339, 2106, 223, 1208, 2441, 2142, 2290, 2534, 1081, 997, 1686, 877, 2650, 1011, 837, 1434, 302, 1010, 2636, 2096, 521, 2426, 2591, 29, 960, 2282, 2100, 2291, 1627, 1596, 1849, 85, 843, 1768, 2, 804, 1969, 567, 274, 773, 281, 633, 63, 425, 465, 566, 1447, 213, 1957, 2629, 1378, 186, 370, 463, 650, 1616, 565, 691, 385, 2494, 601, 1440, 305, 2174, 657, 239, 2666, 2027, 1192, 1828, 1754, 162, 247, 2305, 1137, 483, 1585, 2655, 554, 2007, 1546, 34, 374, 1767, 367, 2432, 412, 1560, 1438, 21, 118, 1845, 933, 1979, 490, 210, 688, 842, 1217, 197, 2408, 1012, 440, 2014, 220, 148, 636, 2647, 564, 807, 868, 92, 502, 1625, 37, 128, 1459, 132, 73, 875, 2051, 252, 739, 306, 473, 568, 277, 1443, 500, 1711, 870, 103, 1841, 173, 1253, 653, 1779, 1515, 2207, 621, 1703, 2551, 563, 1975, 1183, 2309, 2442, 1557, 1649, 39, 294, 1540, 410, 1693, 2368, 2406, 472, 879, 1791, 1954, 35, 738, 321, 1632, 57, 2222, 775, 588, 2453, 1314, 2480, 2586, 1816, 1782, 656, 2209, 1780, 1599, 16, 682, 551, 2482, 937, 1209, 2299, 1377, 541, 862, 2556, 2074, 1416, 986, 1570, 1407, 2465, 928, 783, 817, 147, 2489, 1542, 2568, 1350, 1107, 1354, 2294, 69, 1210, 1987, 1513, 2303, 581, 284, 686, 1665, 49, 2616, 1553, 1830, 543, 1194, 479, 2665, 594, 2581, 1355, 32, 2122, 2245, 1951, 183, 2544, 2652, 2447, 1706, 785, 2038, 1033, 1163, 540, 583, 2593, 266, 670, 1498, 2310, 454, 679, 1084, 2048, 1699, 1597, 1215, 313, 1882, 1653, 1534, 629, 245, 482, 1698, 1126, 414, 1535, 1805, 50, 983, 2060, 2658, 1843, 2645, 1345, 1302, 1214, 166, 1537, 994, 1584, 831, 1626, 2656, 2395, 116, 1248, 2151, 441, 1635, 1002, 2289, 578, 2692, 1123, 231, 1401, 2010, 2547, 1039, 818, 534, 1892, 256, 64, 2372, 1860, 2062, 172, 2399, 2364, 1040, 1877, 593, 2605, 150, 262, 1855, 1475, 2075, 96, 1046, 1642, 303, 1367, 1483, 2057, 300, 947, 1864, 1735, 2052, 1752, 215, 1930, 941, 1207, 1261, 727, 2173, 2255, 2330, 2204, 1717, 448, 1369, 1351, 1257, 721, 2512, 935, 1274, 22, 1410, 934, 530, 1063, 938, 2267, 811, 1572, 1333, 290, 1134, 1950, 2470, 329, 1218, 793, 681, 2704, 1536, 2200, 1734, 2702, 2265, 1331, 1232, 2205, 1441, 2102, 1330, 1565, 1628, 1099, 1127, 2392, 1439, 1965, 361, 1985, 1670, 958, 970, 2082, 1518, 1516, 1054, 1980, 1030, 2250, 1140, 1184, 2703, 974, 2177, 2479, 2206, 139, 2274, 1984, 1117, 1728, 2295, 1594, 1334, 1216, 1077, 2478, 815, 1544, 1110, 1106, 2429, 606, 1016, 743, 1279, 2111, 602, 1424, 1286, 390, 2139, 1204, 243, 2254, 2624, 120, 1719, 1905, 1662, 2519, 2604, 900, 1362, 1590, 2248, 1958, 2664, 1305, 2175, 833, 965, 1159, 2598, 402, 2643, 1473, 1178, 940, 1940, 1959, 1167, 2363, 219, 209, 1675, 2192, 1807, 1408, 2199, 1603, 1474, 1142, 1797, 2055, 1380, 1530, 2264, 1139, 846, 1396, 1466, 1421, 1328, 881, 2611, 1266, 272, 1914, 1772, 1788, 143, 26, 1471, 110, 1787, 1435, 2674, 1952, 192, 1697, 1794, 2413, 730, 311, 2599, 2686, 878, 716, 1340, 579, 437, 393, 2537, 123, 1606, 1451, 1818, 217, 810, 206, 1293, 205, 1912, 2268, 2224, 1462, 1247, 2397, 1144, 349, 1075, 399, 356, 2243, 1871, 845, 122, 1389, 102, 2050, 1295, 1044, 1763, 1252, 254, 520, 214, 2522, 1812, 2477, 503, 1091, 1406, 1893, 1947, 1916, 2390, 2247, 659, 2584, 1038, 2349, 1284, 2203, 677, 36, 1634, 1562, 383, 1203, 763, 2181, 673, 387, 1103, 2569, 2498, 745, 2439, 346, 1789, 1052, 1222, 2230, 930, 731, 616, 914, 2275, 2538, 408, 1411, 685, 678, 1774, 1814, 1243, 330, 2496, 2186, 2672, 2383, 1080, 2427, 328, 2176, 908, 1932, 7, 301, 2576, 2214, 1753, 1755, 241, 2019, 362, 233, 812, 2033, 2535, 2574, 394, 1815, 1157, 946, 1452, 737, 1472, 293, 341, 23, 2421, 2425, 2153, 126, 1419, 768, 2462, 1913, 1287, 275, 211, 1926, 513, 1525, 2069, 1427, 389, 1064, 498, 1682, 1078, 1527, 1876, 352, 1641, 558, 2063, 280, 95, 729, 1614, 2094, 2266, 1414, 1372, 742, 2313, 1765, 1425, 1868, 904, 1415, 41, 684, 1945, 1666, 704, 2566, 894, 2064, 2474, 419, 619, 748, 81, 250, 1938, 1716, 185, 2417, 1393, 2377, 1268, 1532, 2380, 2514, 2634, 2463, 639, 898, 115, 697, 645, 2401, 2138, 2679, 647, 2008, 94, 1898, 93, 1870, 1172, 392, 2677, 345, 1506, 170, 2115, 2588, 1255, 2293, 1731, 2292, 108, 1458, 772, 161, 852, 2081, 2166, 510, 976, 855, 892, 557, 2698, 1761, 2241, 1908, 358, 1241, 2529, 2419, 2434, 2242, 2234, 2219, 2464, 1308, 2343, 76, 450, 1901, 1162, 1764, 2654, 888, 641, 1582, 723, 182, 1013, 2097, 1937, 1549, 292, 117, 2510, 2366, 1602, 423, 1777, 2461, 1545, 2443, 2575, 1747, 1899, 52, 574, 1310, 1329, 1762, 1918, 1449, 1199, 836, 1390, 2644, 2346, 2073, 786, 569, 453, 1433, 555, 1223, 1484, 1197, 571, 1567, 2511, 2246, 2528, 700, 381, 2475, 931, 1269, 1135, 459, 2467, 2345, 2168, 1006, 1337, 1510, 199, 296, 2036, 177, 2080, 2418, 525, 1004, 1482, 798, 550, 1189, 2627, 436, 1712, 2198, 1858, 438, 1360, 1273, 70, 967, 323, 522, 1074, 1024, 1727, 1505, 1967, 25, 2110, 1595, 2342, 6, 556, 114, 1468, 1986, 2696, 2156, 251, 1109, 2320, 151, 2324, 20, 2476, 1823, 507, 308, 1521, 1835, 1561, 378, 1658, 171, 876, 2040, 1460, 424, 208, 2487, 2197, 640, 2675, 1191, 2005, 2350, 1824, 2210, 1671, 695, 2182, 1444, 382, 1270, 140, 2689, 319, 1238, 1181, 1237, 1861, 1888, 2423, 164, 1235, 2225, 1647, 488, 2623, 1491, 109, 1227, 79, 2329, 1174, 264, 1095, 740, 2099, 887, 9, 706, 1053, 2059, 2134, 2092, 865, 903, 926]
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [2708, 7]                 --
├─Sequential_c81df2: 1-1                 [2708, 7]                 --
│    └─GCNConv: 2-1                      [2708, 50]                50
│    │    └─Linear: 3-1                  [2708, 50]                71,650
│    │    └─SumAggregation: 3-2          [2708, 50]                --
│    └─ReLU: 2-2                         [2708, 50]                --
│    └─GCNConv: 2-3                      [2708, 7]                 7
│    │    └─Linear: 3-3                  [2708, 7]                 350
│    │    └─SumAggregation: 3-4          [2708, 7]                 --
│    └─Sigmoid: 2-4                      [2708, 7]                 --
==========================================================================================
Total params: 72,057
Trainable params: 72,057
Non-trainable params: 0
Total mult-adds (M): 194.98
==========================================================================================
Input size (MB): 15.61
Forward/backward pass size (MB): 1.23
Params size (MB): 0.29
Estimated Total Size (MB): 17.13
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.10782865583456426 Loss: 1.9447355270385742
Epoch: 100 Accuracy: 0.5258493353028065 Loss: 1.8176501989364624
Epoch: 200 Accuracy: 0.6824224519940916 Loss: 1.679567575454712
Epoch: 300 Accuracy: 0.7211964549483013 Loss: 1.5665239095687866
Epoch: 400 Accuracy: 0.7370753323485968 Loss: 1.4733980894088745
Epoch: 500 Accuracy: 0.741506646971935 Loss: 1.4004459381103516
Epoch: 600 Accuracy: 0.7426144756277696 Loss: 1.3460379838943481
Epoch: 700 Accuracy: 0.741506646971935 Loss: 1.3061797618865967
Epoch: 800 Accuracy: 0.740029542097489 Loss: 1.2769527435302734
Epoch: 900 Accuracy: 0.742245199409158 Loss: 1.2552289962768555
Epoch: 1000 Accuracy: 0.7403988183161004 Loss: 1.2388395071029663
Epoch: 1100 Accuracy: 0.740029542097489 Loss: 1.2262749671936035
Epoch: 1200 Accuracy: 0.7392909896602659 Loss: 1.21648108959198
Epoch: 1300 Accuracy: 0.7374446085672083 Loss: 1.2087384462356567
Epoch: 1400 Accuracy: 0.7385524372230429 Loss: 1.202528953552246
Epoch: 1500 Accuracy: 0.7385524372230429 Loss: 1.1974860429763794
Epoch: 1600 Accuracy: 0.7378138847858198 Loss: 1.1933459043502808
Epoch: 1700 Accuracy: 0.7374446085672083 Loss: 1.1899107694625854
Epoch: 1800 Accuracy: 0.7370753323485968 Loss: 1.187034010887146
Epoch: 1900 Accuracy: 0.7359675036927622 Loss: 1.1846070289611816
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.93      0.70      0.80       150
           1       0.85      0.79      0.82       150
           2       0.60      0.84      0.70       150
           3       0.81      0.89      0.85       150
           4       0.73      0.63      0.67       150
           5       0.77      0.76      0.76       150
           6       0.67      0.64      0.66       150

    accuracy                           0.75      1050
   macro avg       0.76      0.75      0.75      1050
weighted avg       0.76      0.75      0.75      1050

saving models...
Accurarcy is: 75.04761904761905%
