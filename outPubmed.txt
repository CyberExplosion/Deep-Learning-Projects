Loading pubmed dataset...
Unique labels [0 1 2]
The index train: [17631, 1319, 16273, 10506, 6208, 7463, 18296, 7195, 15079, 7027, 3603, 2711, 3398, 8201, 4918, 548, 17067, 2594, 9439, 10153, 13414, 7820, 8601, 4214, 6088, 10147, 3076, 10261, 14270, 3351, 19556, 10113, 17942, 8530, 1550, 11349, 6499, 16667, 12303, 9154, 4001, 17659, 878, 15460, 958, 10705, 2235, 18549, 4233, 7443, 4899, 8310, 18636, 17152, 3552, 3332, 4878, 6918, 793, 16422] and index test: [12111, 2801, 7460, 10878, 12144, 19555, 18934, 1986, 14876, 9212, 11389, 13465, 15639, 7774, 6063, 3425, 10218, 13571, 19708, 8614, 7962, 16662, 10993, 16002, 4413, 15216, 5835, 8253, 17749, 13941, 13335, 7556, 1552, 9551, 1192, 7558, 9830, 4546, 18560, 5047, 14629, 18929, 9688, 17694, 7984, 6340, 14646, 18532, 1818, 10936, 5503, 1368, 16736, 11674, 1235, 19490, 812, 4945, 5802, 66, 11690, 13112, 5652, 5353, 5396, 13604, 7006, 16287, 6963, 825, 16086, 2697, 1464, 1430, 14667, 15140, 4038, 16516, 11004, 7248, 660, 13656, 15491, 7314, 3053, 7079, 17136, 3023, 6795, 17755, 893, 15295, 5349, 5608, 12069, 6058, 860, 12058, 17181, 2916, 14639, 13917, 5762, 12597, 16275, 5178, 9413, 11580, 19272, 4423, 10633, 12053, 13673, 14160, 19251, 17931, 13516, 258, 6526, 14825, 14201, 1519, 19707, 4629, 18927, 7320, 8589, 11282, 12202, 3343, 19016, 13006, 18319, 15152, 8246, 15721, 11207, 320, 3596, 3828, 19078, 107, 6637, 7927, 13547, 19485, 5611, 13467, 8077, 14123, 4855, 13693, 17319, 12948, 1971, 11961, 17622, 242, 6332, 1278, 15354, 11598, 8079, 2746, 17147, 15467, 6495, 9959, 19345, 11234, 5820, 3710, 17245, 1902, 3410, 3148, 11536, 6729, 7094, 12317, 5309, 10175, 6365, 7797, 6098, 2519, 17340, 15528, 13495, 11531, 6075, 14610, 14729, 8683, 14925, 2753, 12887, 3472, 375, 1667, 9665, 8941, 9650, 15, 10084, 7053, 6091, 13939, 6778, 11574, 1022, 6588, 17405, 7561, 19672, 13438, 977, 13586, 15180, 13351, 8282, 9339, 16350, 18758, 17360, 18821, 3420, 14574, 9951, 19327, 12136, 1310, 8072, 3816, 15107, 1516, 5052, 9684, 8810, 16403, 10247, 484, 12407, 12844, 18665, 15833, 17334, 13478, 2794, 12188, 11730, 1086, 8572, 12749, 2333, 13386, 8135, 5150, 15666, 2933, 14662, 19435, 8241, 7891, 18461, 14424, 4015, 7764, 14861, 8355, 12893, 12834, 1719, 966, 3339, 14669, 10388, 6522, 507, 6864, 18013, 16250, 6202, 2864, 12641, 14258, 18097, 11038, 7810, 18370, 12134, 6357, 16362, 14498, 3063, 14255, 18520, 6238, 706, 10566, 3164, 17567, 10876, 15109, 15125, 17033, 17551, 1085, 18443, 5827, 2923, 141, 11981, 17063, 15975, 7135, 15830, 3426, 12826, 19027, 14437, 169, 59, 13223, 835, 6194, 8496, 1931, 3088, 3361, 6285, 17623, 3229, 16265, 10640, 9700, 2087, 17221, 15534, 11750, 4372, 18886, 15367, 13885, 3147, 334, 5867, 12874, 3783, 10079, 18630, 16167, 13441, 16776, 528, 2732, 13838, 17561, 18584, 4398, 18426, 4147, 4399, 6926, 17179, 1511, 10029, 12154, 5887, 16775, 11277, 9645, 10847, 6640, 745, 15056, 6134, 10666, 13008, 17440, 8999, 8937, 6782, 16334, 1004, 15868, 8460, 7539, 7015, 15119, 15201, 4063, 15432, 7487, 10409, 3106, 11691, 3263, 6225, 5467, 30, 9628, 10030, 1099, 2738, 7139, 4488, 16450, 19140, 10812, 16241, 19347, 10986, 13494, 7575, 2489, 11614, 11582, 7386, 7211, 3626, 808, 11215, 2141, 16684, 266, 4479, 12543, 14458, 3276, 12833, 1260, 18957, 14483, 3237, 354, 9972, 8777, 13934, 16936, 14554, 15269, 6174, 14321, 12852, 19651, 6245, 3040, 6519, 8237, 18518, 17115, 1068, 17227, 15873, 5465, 16342, 5255, 2046, 18243, 15733, 7719, 6036, 1151, 8268, 2504, 7276, 12331, 11078, 4932, 17799, 8788, 12601, 14900, 16024, 5825, 2901, 1599, 2929, 5781, 8832, 3382, 14552, 12855, 13764, 8744, 11516, 1173, 15523, 4709, 13985, 16057, 5259, 6228, 15256, 4305, 14261, 3925, 4071, 3305, 325, 7860, 11049, 15445, 19349, 17898, 291, 12175, 6846, 14276, 18799, 1018, 13807, 120, 3116, 10762, 18727, 12664, 11213, 5312, 4935, 11147, 8445, 13319, 17089, 5251, 16723, 17877, 8798, 12822, 15040, 17412, 15132, 15159, 12890, 821, 19639, 16056, 2518, 945, 16319, 12330, 12257, 13570, 1701, 15935, 8556, 4788, 9569, 6467, 7456, 5346, 9539, 16461, 13852, 14607, 11667, 13052, 7914, 15679, 3433, 11373, 2334, 10770, 18076, 19031, 15399, 6384, 18177, 1139, 10300, 13294, 10651, 13513, 10678, 168, 3927, 5062, 11430, 8132, 1234, 1363, 10940, 1279, 17949, 3542, 17431, 18152, 1914, 15767, 17929, 1133, 10171, 2037, 490, 8481, 7075, 5914, 15291, 7220]
method running...
--network status--
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
Method_Classification                    [19717, 3]                --
├─Sequential_87497e: 1-1                 [19717, 3]                --
│    └─GCNConv: 2-1                      [19717, 100]              100
│    │    └─Linear: 3-1                  [19717, 100]              50,000
│    │    └─SumAggregation: 3-2          [19717, 100]              --
│    └─ReLU: 2-2                         [19717, 100]              --
│    └─GCNConv: 2-3                      [19717, 50]               50
│    │    └─Linear: 3-3                  [19717, 50]               5,000
│    │    └─SumAggregation: 3-4          [19717, 50]               --
│    └─ReLU: 2-4                         [19717, 50]               --
│    └─GCNConv: 2-5                      [19717, 16]               16
│    │    └─Linear: 3-5                  [19717, 16]               800
│    │    └─SumAggregation: 3-6          [19717, 16]               --
│    └─ReLU: 2-6                         [19717, 16]               --
│    └─Linear: 2-7                       [19717, 3]                51
│    └─Sigmoid: 2-8                      [19717, 3]                --
==========================================================================================
Total params: 56,017
Trainable params: 56,017
Non-trainable params: 0
Total mult-adds (G): 1.10
==========================================================================================
Input size (MB): 40.14
Forward/backward pass size (MB): 26.66
Params size (MB): 0.22
Estimated Total Size (MB): 67.02
==========================================================================================
--start training...
Epoch: 0 Accuracy: 0.20809453770857636 Loss: 1.0993536710739136
Epoch: 100 Accuracy: 0.20880458487599535 Loss: 1.08661949634552
Epoch: 200 Accuracy: 0.46766749505502864 Loss: 1.0361478328704834
Epoch: 300 Accuracy: 0.6188061064056398 Loss: 0.9371546506881714
Epoch: 400 Accuracy: 0.6496424405335497 Loss: 0.8396590352058411
Epoch: 500 Accuracy: 0.6780443272303088 Loss: 0.7546167373657227
Epoch: 600 Accuracy: 0.693310341329817 Loss: 0.6807986497879028
Epoch: 700 Accuracy: 0.7021859309225541 Loss: 0.63088059425354
Epoch: 800 Accuracy: 0.701780189684029 Loss: 0.6008398532867432
Epoch: 900 Accuracy: 0.6992443069432469 Loss: 0.5834053158760071
Epoch: 1000 Accuracy: 0.6957447887609677 Loss: 0.5730117559432983
Epoch: 1100 Accuracy: 0.6934624942942639 Loss: 0.5665971636772156
Epoch: 1200 Accuracy: 0.6916873763757164 Loss: 0.5624879598617554
Epoch: 1300 Accuracy: 0.6907237409342192 Loss: 0.5597530007362366
Epoch: 1400 Accuracy: 0.6898615408023533 Loss: 0.5578685402870178
Epoch: 1500 Accuracy: 0.6897093878379064 Loss: 0.5565271377563477
Epoch: 1600 Accuracy: 0.6897093878379064 Loss: 0.5555448532104492
Epoch: 1700 Accuracy: 0.6888979053608562 Loss: 0.5548080205917358
Epoch: 1800 Accuracy: 0.6884921641223309 Loss: 0.5542422533035278
Epoch: 1900 Accuracy: 0.6881371405386215 Loss: 0.5537994503974915
Epoch: 2000 Accuracy: 0.6882385758482528 Loss: 0.5534470081329346
Epoch: 2100 Accuracy: 0.6879342699193589 Loss: 0.5531625747680664
Epoch: 2200 Accuracy: 0.6879342699193589 Loss: 0.5529307126998901
Epoch: 2300 Accuracy: 0.6879342699193589 Loss: 0.5527392029762268
Epoch: 2400 Accuracy: 0.6880357052289902 Loss: 0.5525792241096497
Epoch: 2500 Accuracy: 0.6878835522645433 Loss: 0.5524444580078125
Epoch: 2600 Accuracy: 0.6875792463356495 Loss: 0.5523301362991333
Epoch: 2700 Accuracy: 0.6875285286808338 Loss: 0.5522323250770569
Epoch: 2800 Accuracy: 0.6875285286808338 Loss: 0.552148163318634
Epoch: 2900 Accuracy: 0.6875285286808338 Loss: 0.5520753264427185
--start testing...
run performace metrics: 
              precision    recall  f1-score   support

           0       0.75      0.48      0.59       200
           1       0.73      0.85      0.79       200
           2       0.66      0.79      0.72       200

    accuracy                           0.71       600
   macro avg       0.71      0.71      0.70       600
weighted avg       0.71      0.71      0.70       600

saving models...
Accurarcy is: 70.83333333333334%
