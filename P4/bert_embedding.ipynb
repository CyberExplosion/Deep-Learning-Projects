{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tryout bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.', 1)\n"
     ]
    }
   ],
   "source": [
    "# combine both negative and pos in train and test to train\n",
    "dataSet = {}\n",
    "dataSet['train'] = []\n",
    "dataSet['test'] = []\n",
    "for p in Path('data/text_classification/train/pos/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['train'].append((entry, 1))\n",
    "for p in Path('data/text_classification/train/neg/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['train'].append((entry, 0))\n",
    "\n",
    "for p in Path('data/text_classification/test/pos/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['test'].append((entry, 1))\n",
    "for p in Path('data/text_classification/test/neg/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['test'].append((entry, 0))\n",
    "\n",
    "\n",
    "print(dataSet['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# with bert we don't need to split the sentences. But do remove the html\n",
    "cleanedTrainData = []\n",
    "CLEANHTML = re.compile('<.*?>')\n",
    "\n",
    "for entry in dataSet['train']:\n",
    "    # ! Hope the data doesn't contain heavy html tags or else it wouldn't work\n",
    "    text = re.sub(CLEANHTML, '', entry[0])\n",
    "    cleanedTrainData.append((text, entry[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.\n",
      "When this movie first came out back in 1984, Prince was one of the hottest acts around. Everyone wanted to see this movie, which was not much more than a extended music video. The acting was pretty bad, but what can you expect from musicians acting on the big screen for the first time? Despite that, it was still a very entertaining film! Morris Day and Jerome Benton provide some all time classic comedy, especially their rendition of \"The Password\", which will make you think of Abbott & Costello doing their \"who's on first\" baseball routine.Appolina (who went by a single name then) provided some beautiful breasts, so you had the brief nudity covered. Plus, she is very attractive. And of course, the soundtrack of the album is one of the best Prince ever recorded. Prince later on had a fallout with Warner Bros. and changed his name, but at this particular time in his career, he was at the top of his game.This movie doesn't rank in the all time great category, but it is pretty entertaining.\n",
      "I watched the Pie-lette last night and the word that comes to mind is \"original.\" It is a word not used much in TV as they all tend to copy whatever the other network is doing and you end up with seven nights of crime shows, unfunny comedies, and reality crap.The first thing that hit me like a brick was the presence of Jim Dale. Those not familiar with the British \"Carry On ...\" series or those who have not listened to a Harry Potter book, may not be familiar with Dale. I am not sure whether his presence as narrator adds or distracts. I will have to tune in more, but it does give the show a \"Harry Potter\" atmosphere. Maybe that's a good thing.Lee Pace (Infamous, The White Countess) has a gift. It never explains where he got it, but he can bring someone back from the dead for a minute. He teams with Chi McBride (\"Boston Public,\" Roll Bounce) to solve murders using this talent. Everything is fine and funny until he comes across a childhood love, Anna Friel (Goal! The Dream Begins, Timeline) and things really get complicated. He can't send her back and he can never touch her. Boy, would that make a relationship difficult.I will be tuning in to see where this series goes in the expectation that it will continue to entertain.\n"
     ]
    }
   ],
   "source": [
    "# try print out a cleaned text\n",
    "for i in range(0, 3):\n",
    "    print(cleanedTrainData[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Add special required formatting for BERT\n",
    "# maxSentenceLen = 0\n",
    "# for text, label in cleanedTrainData:\n",
    "#     input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "# \n",
    "#     maxSentenceLen = max(maxSentenceLen, len(input_ids))\n",
    "# \n",
    "# print(f'The max sentences length is: {maxSentenceLen}')\n",
    "\n",
    "# ! BERT only take 512 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# test truncation methods [cut at the end -> shown to give better result]\n",
    "# ? https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification?noredirect=1&lq=1\n",
    "# ? https://github.com/huggingface/transformers/issues/4476#issuecomment-951445067\n",
    "def encode_right_truncated(tokenizer, text, padding='max_length', max_length=512, add_special_tokens=True):\n",
    "    out = tokenizer(text, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "    tokenized = out['input_ids']\n",
    "    print(f\"Length of the inputID: {len(tokenized)}, length of token type: {len(out['token_type_ids'])}, length of mask: {len(out['attention_mask'])}\")\n",
    "    print(f\"type: {out['token_type_ids']}\")\n",
    "    print(f\"type: {out['attention_mask']}\")\n",
    "    if not add_special_tokens:\n",
    "        truncated = tokenized[-max_length:]\n",
    "    else:\n",
    "        truncated = tokenized[0:1] + tokenized[-(max_length-1):]    # keep special start and end symbol\n",
    "    \n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the inputID: 512, length of token type: 512, length of mask: 512\n",
      "type: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "type: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 6551, 5632, 2023, 3386, 6547, 10874, 2143, 2055, 1037, 2402, 2450, 1010, 9401, 1042, 11663, 1010, 1006, 6423, 5811, 1007, 2040, 2003, 2041, 1997, 2147, 1998, 2038, 5357, 2369, 1999, 2014, 9278, 1998, 2003, 7143, 2000, 2424, 2147, 1012, 6423, 9631, 2019, 4748, 1999, 1996, 2334, 2414, 3780, 2559, 2005, 1037, 3187, 1998, 18545, 2041, 2000, 3046, 1998, 6855, 2023, 2597, 1012, 6423, 6855, 2015, 1996, 2597, 1998, 2003, 5086, 2011, 1037, 3680, 1012, 8099, 1010, 1006, 8214, 2089, 25591, 1007, 2040, 5942, 2008, 2016, 3268, 2007, 2014, 11194, 1999, 2014, 2188, 1998, 4122, 2014, 2000, 2031, 2053, 6624, 2007, 2273, 2814, 1998, 6423, 4136, 2068, 2016, 2038, 2053, 2155, 1998, 2003, 2489, 2000, 23313, 2014, 2972, 2051, 2000, 2023, 3105, 1012, 2577, 6097, 16416, 5149, 1010, 1006, 6798, 8099, 1007, 2003, 1996, 2365, 1997, 3680, 1012, 8099, 1998, 2038, 2070, 2200, 4326, 14714, 2005, 2652, 2105, 2007, 13227, 1012, 2023, 2001, 1037, 2659, 5166, 2143, 1998, 2087, 1997, 1996, 5019, 2020, 2485, 11139, 1999, 2344, 2000, 4468, 1996, 10961, 1997, 1037, 4281, 1998, 5366, 2005, 17363, 1012, 2023, 4326, 2155, 2035, 2444, 1999, 1037, 4121, 7330, 2125, 1996, 10387, 3023, 1997, 2563, 1998, 2045, 2003, 3595, 4303, 1998, 7564, 1997, 23873, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Keeping only 512 word at the end\n",
    "count = 0\n",
    "\n",
    "for text, label in cleanedTrainData:\n",
    "    input_ids = encode_right_truncated(tokenizer, text)\n",
    "    print(input_ids)\n",
    "    break\n",
    "# print(f'The max sentences length is: {maxSentenceLen}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# unpacking\n",
    "wow = {\n",
    "    'a': 1,\n",
    "    'b': 20\n",
    "}\n",
    "\n",
    "def func (a, b):\n",
    "    print(a, b)\n",
    "\n",
    "# func(**{'a': 1, 'b': 2}, **{'c':4, 'd':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# LOOKING GOOD, now we can reuse the previous loop to create the input tensor\n",
    "\n",
    "def encode_right_truncated(tokenizer, text, padding='max_length', max_length=512, add_special_tokens=True):\n",
    "    out = tokenizer(text, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "    tokenized = out['input_ids']\n",
    "    tokenType = out['token_type_ids']\n",
    "    attention = out['attention_mask']\n",
    "\n",
    "    if not add_special_tokens:\n",
    "        truncated = {\n",
    "            'input_ids': tokenized[-max_length:],\n",
    "            'token_type_ids': tokenType[-max_length:],\n",
    "            'attention_mask': attention[-max_length:],\n",
    "        }\n",
    "    else:\n",
    "        truncated = {\n",
    "            'input_ids': tokenized[0:1] + tokenized[-(max_length-1):],    # keep special start and end symbol\n",
    "            'token_type_ids': tokenType[0:1] + tokenType[-(max_length-1):],\n",
    "            'attention_mask': attention[0:1] + attention[-(max_length-1):],\n",
    "        }\n",
    "    \n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputID: torch.Size([512]) and shape of attention: torch.Size([512]) and label shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "testText = cleanedTrainData[0][0]\n",
    "test = encode_right_truncated(tokenizer, testText)\n",
    "\n",
    "inputIDTensor = torch.tensor(test['input_ids'])\n",
    "attentionTensor = torch.tensor(test['attention_mask'])\n",
    "labelTensor = torch.tensor(cleanedTrainData[0][1])\n",
    "\n",
    "print(f'shape of inputID: {inputIDTensor.shape} and shape of attention: {attentionTensor.shape} and label shape: {labelTensor.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input of RNN should be tensor of (batch size, max length for the whole text, dimesion of the vocab vector [the size of the vector use to represent the vocab]) \n",
    "### in Bert, the shape is (batch, 512, 768) -> 768 is the size of each vocab vector in BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out putting test_data into Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data....\n",
      "Some output: \n",
      "[('\"All the world\\'s a stage and its people actors in it\"--or something like that. Who the hell said that theatre stopped at the orchestra pit--or even at the theatre door? Why is not the audience participants in the theatrical experience, including the story itself?This film was a grand experiment that said: \"Hey! the story is you and it needs more than your attention, it needs your active participation\". \"Sometimes we bring the story to you, sometimes you have to go to the story.\"Alas no one listened, but that does not mean it should not have been said.', 1), (\"FUTZ is the only show preserved from the experimental theatre movement in New York in the 1960s (the origins of Off Off Broadway). Though it's not for everyone, it is a genuinely brilliant, darkly funny, even more often deeply disturbing tale about love, sex, personal liberty, and revenge, a serious morality tale even more relevant now in a time when Congress wants to outlaw gay marriage by trashing our Constitution. The story is not about being gay, though -- it's about love and sex that don't conform to social norms and therefore must be removed through violence and hate. On the surface, it tells the story of a man who falls in love with a pig, but like any great fable, it's not really about animals, it's about something bigger -- stifling conformity in America.The stage version won international acclaim in its original production, it toured the U.S. and Europe, and with others of its kind, influenced almost all theatre that came after it. Luckily, we have preserved here the show pretty much as it was originally conceived, with the original cast and original director, Tom O'Horgan (who also directed HAIR and Jesus Christ Superstar on Broadway).This is not a mainstream, easy-to-take, studio film -- this is an aggressive, unsettling, glorious, deeply emotional, wildly imaginative piece of storytelling that you'll never forget. And it just might change the way you see the world...\", 1)]\n"
     ]
    }
   ],
   "source": [
    "processedData = {\n",
    "    'train': [],\n",
    "    'test': []\n",
    "}\n",
    "\n",
    "# * 1 is for positive, 0 is negative\n",
    "print('loading data....')\n",
    "dataPath = 'data/test_data'\n",
    "\n",
    "for p in Path(dataPath, 'train', 'pos').glob('*.txt'):  # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    processedData['train'].append((entry, 1))\n",
    "for p in Path(dataPath, 'train', 'neg').glob('*.txt'):  # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    processedData['train'].append((entry, 0))\n",
    "\n",
    "for p in Path(dataPath, 'test', 'pos').glob('*.txt'):  # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    processedData['test'].append((entry, 1))\n",
    "for p in Path(dataPath, 'test', 'neg').glob('*.txt'):  # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    processedData['test'].append((entry, 0))\n",
    "\n",
    "# Clean the training data\n",
    "cleanedData = []\n",
    "CLEANHTML = re.compile('<.*?>')\n",
    "\n",
    "for entry in processedData['train']:\n",
    "    # ! Hope the data doesn't contain heavy html tags or else it wouldn't work\n",
    "    text = re.sub(CLEANHTML, '', entry[0])\n",
    "    cleanedData.append((text, entry[1]))\n",
    "\n",
    "\n",
    "processedData['train'] = cleanedData\n",
    "print('Some output: ')\n",
    "print(processedData['train'][0:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Embedding is just one step to get the embedded vector before putting into the model\n",
    "Bert vector already been trained to have word related to each other to be close in their space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Testing out converting data into tensor shape\n",
    "inputData = {\n",
    "    'train': {\n",
    "        'X': [],\n",
    "        'y': []\n",
    "    },\n",
    "    'test': {\n",
    "        'X': [],\n",
    "        'y': []\n",
    "    }\n",
    "}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BERT_MAX_LENGTH = 512\n",
    "\n",
    "for i, (text, label) in enumerate(processedData['train']):\n",
    "    out = tokenizer(text, padding='max_length',\n",
    "                    max_length=BERT_MAX_LENGTH, add_special_tokens=True)\n",
    "    tokenized = out['input_ids']\n",
    "    tokenType = out['token_type_ids']\n",
    "    attention = out['attention_mask']\n",
    "    truncated = {\n",
    "        # keep special start and end symbol\n",
    "        'input_ids': tokenized[0:1] + tokenized[-(BERT_MAX_LENGTH-1):],\n",
    "        'token_type_ids': tokenType[0:1] + tokenType[-(BERT_MAX_LENGTH-1):],\n",
    "        'attention_mask': attention[0:1] + attention[-(BERT_MAX_LENGTH-1):],\n",
    "    }\n",
    "    inputData['train']['X'].append(truncated)\n",
    "    inputData['train']['y'].append(label)\n",
    "\n",
    "for i, (text, label) in enumerate(processedData['test']):\n",
    "    out = tokenizer(text, padding='max_length',\n",
    "                    max_length=BERT_MAX_LENGTH, add_special_tokens=True)\n",
    "    tokenized = out['input_ids']\n",
    "    tokenType = out['token_type_ids']\n",
    "    attention = out['attention_mask']\n",
    "    truncated = {\n",
    "        # keep special start and end symbol\n",
    "        'input_ids': tokenized[0:1] + tokenized[-(BERT_MAX_LENGTH-1):],\n",
    "        'token_type_ids': tokenType[0:1] + tokenType[-(BERT_MAX_LENGTH-1):],\n",
    "        'attention_mask': attention[0:1] + attention[-(BERT_MAX_LENGTH-1):],\n",
    "    }\n",
    "    inputData['test']['X'].append(truncated)\n",
    "    inputData['test']['y'].append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data len: 10, test label len: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test data len: {len(inputData['test']['X'])}, test label len: {len(inputData['test']['y'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [2, 2, 2,  ..., 2, 2, 2],\n",
       "        ...,\n",
       "        [7, 7, 7,  ..., 7, 7, 7],\n",
       "        [8, 8, 8,  ..., 8, 8, 8],\n",
       "        [9, 9, 9,  ..., 9, 9, 9]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BERT is trained and expect sentence pairs, so we need to number each tensor to belong to a text\n",
    "segments_ids = [[x] * 512 for x in range(0, len(inputData['train']['X']))]\n",
    "segments_tensor = torch.tensor(segments_ids)\n",
    "print(segments_tensor.shape)\n",
    "segments_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######## Extracting embeddings\n",
    "\n",
    "# Convert inputs to pytorch tensor\n",
    "tokens_list = []\n",
    "for each in inputData['train']['X']:\n",
    "    tokens_list.append(each['input_ids'])\n",
    "tokens_tensor = torch.tensor(tokens_list)\n",
    "\n",
    "tokens_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the BERT embedding model\n",
    "bertModel = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "bertModel.eval()    # Only wants to use the bert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Now get the Bert hidden state vector (high quality) from Bert output so we can use in our own model\n",
    "with torch.no_grad():\n",
    "    outputs = bertModel(tokens_tensor, segments_tensor)\n",
    "\n",
    "    hidden_states = outputs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.5467,  0.3301, -0.9227,  ...,  0.9150,  0.8351, -0.2478],\n",
       "          [-0.8933,  0.4482, -0.2173,  ...,  0.8924,  0.5684, -0.6553],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]],\n",
       " \n",
       "         [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.5402, -0.0159, -0.3557,  ..., -0.3442,  0.2522, -0.2073],\n",
       "          [-1.7013, -0.3745, -0.0449,  ...,  0.8165,  1.0988,  0.6134],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]],\n",
       " \n",
       "         [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.3406,  0.7025, -0.6483,  ...,  0.2240,  0.7512,  0.2386],\n",
       "          [-0.6288,  0.4486,  0.6314,  ...,  0.5974,  0.5314,  0.2476],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.4389, -0.8401, -0.6736,  ...,  0.7098,  0.5383, -0.9859],\n",
       "          [-0.6860,  0.4399,  0.2051,  ...,  0.4881,  0.3567,  0.2992],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]],\n",
       " \n",
       "         [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [ 0.3980, -0.0393, -0.2927,  ...,  0.8112, -0.2715, -1.1264],\n",
       "          [-1.1132, -0.0312, -0.0827,  ...,  0.3541,  1.6199, -0.7420],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]],\n",
       " \n",
       "         [[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "          [-0.6485,  0.6739, -0.0932,  ...,  0.4475,  0.6696,  0.1820],\n",
       "          [ 1.0736, -0.8878,  0.2915,  ...,  0.3606, -0.6414, -0.0800],\n",
       "          ...,\n",
       "          [ 0.6458, -0.5409, -0.1780,  ..., -0.0482, -0.3466, -0.4825],\n",
       "          [ 0.7417, -0.7271,  0.3278,  ..., -0.2011, -0.6038, -0.4935],\n",
       "          [ 0.2992, -1.0338,  0.1294,  ...,  0.2149,  0.2113, -1.5097]]]),\n",
       " tensor([[[ 0.2172, -0.3201, -0.0114,  ..., -0.0189,  0.4015,  0.2714],\n",
       "          [ 0.4436, -0.6034, -0.5862,  ...,  1.0023,  0.8752,  0.1156],\n",
       "          [-0.8694, -0.3676, -0.0201,  ...,  0.7294,  1.3999, -0.1906],\n",
       "          ...,\n",
       "          [ 0.6522, -0.3654, -0.0912,  ...,  0.4378,  0.1676, -0.4692],\n",
       "          [ 0.7608, -0.5687,  0.3079,  ...,  0.3557, -0.1026, -0.3777],\n",
       "          [ 0.4315, -0.8725,  0.3081,  ...,  0.4991,  0.5032, -1.3740]],\n",
       " \n",
       "         [[ 0.0092, -0.0617, -0.1918,  ...,  0.3430, -0.1099,  0.0297],\n",
       "          [ 0.6367, -0.6252, -0.3704,  ..., -0.2565,  0.6602, -0.0726],\n",
       "          [-2.1511, -1.0969,  0.1407,  ...,  0.5414,  1.1092,  1.5007],\n",
       "          ...,\n",
       "          [ 0.4990, -0.1693, -0.3392,  ...,  0.3255, -0.0514, -0.3798],\n",
       "          [ 0.6284, -0.4553,  0.0650,  ...,  0.3116, -0.2797, -0.3071],\n",
       "          [ 0.2319, -0.7538,  0.0790,  ...,  0.5187,  0.3202, -1.3041]],\n",
       " \n",
       "         [[ 0.2451, -0.3561, -0.0396,  ...,  0.0158,  0.3388,  0.2544],\n",
       "          [-0.3052,  0.1688, -0.6267,  ...,  0.2449,  0.5819,  0.5211],\n",
       "          [-1.1743, -0.4079,  0.5727,  ...,  0.7736,  0.7234,  0.3548],\n",
       "          ...,\n",
       "          [ 0.6400, -0.3716, -0.0665,  ...,  0.5127,  0.1614, -0.4758],\n",
       "          [ 0.7513, -0.5722,  0.3328,  ...,  0.4322, -0.0935, -0.3953],\n",
       "          [ 0.4211, -0.8695,  0.3373,  ...,  0.6068,  0.5162, -1.4050]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[-0.1316, -0.8111,  0.1815,  ...,  0.0796, -0.0623,  0.3187],\n",
       "          [ 0.0963, -1.1665, -0.4301,  ...,  1.1183,  0.2586,  0.0181],\n",
       "          [-1.2039, -0.4968,  0.1682,  ...,  0.1885,  0.6831,  0.0499],\n",
       "          ...,\n",
       "          [ 0.6045, -1.1662,  0.1710,  ...,  0.7291,  0.0216, -0.5086],\n",
       "          [ 0.7224, -1.4584,  0.6371,  ...,  0.6686, -0.4581, -0.3697],\n",
       "          [ 0.0314, -1.8133,  0.7975,  ...,  0.9077,  0.4955, -1.6692]],\n",
       " \n",
       "         [[ 0.0096, -0.2944, -0.3218,  ...,  0.3737, -0.1285,  0.0730],\n",
       "          [ 0.9504, -0.8016, -0.4166,  ..., -0.0907,  0.7663,  0.2469],\n",
       "          [-2.2035, -1.2602, -0.0941,  ...,  0.4303,  1.5020,  1.7478],\n",
       "          ...,\n",
       "          [ 0.8679, -0.3524,  0.0960,  ...,  0.3951,  0.4599, -0.7439],\n",
       "          [ 1.1235, -0.8075,  0.6128,  ...,  0.4101,  0.0670, -0.5957],\n",
       "          [ 0.2088, -1.3075,  0.8806,  ...,  0.8334,  0.7816, -1.8375]],\n",
       " \n",
       "         [[-0.1123, -0.7572,  0.1477,  ...,  0.0696, -0.0966,  0.3059],\n",
       "          [-0.5559,  0.2658, -0.4815,  ...,  0.2994, -0.1660,  0.7552],\n",
       "          [-1.7901, -1.0429,  0.4499,  ...,  0.9110,  0.2520,  0.6683],\n",
       "          ...,\n",
       "          [ 0.5840, -1.0553,  0.2658,  ...,  0.8273,  0.0248, -0.5390],\n",
       "          [ 0.7207, -1.3615,  0.7477,  ...,  0.7873, -0.4685, -0.4178],\n",
       "          [ 0.0351, -1.7446,  0.8750,  ...,  1.0126,  0.4995, -1.7292]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[-8.8728e-02, -4.4978e-01,  1.8071e-01,  ...,  7.6576e-02,\n",
       "           -2.0189e-01,  3.5198e-01],\n",
       "          [ 1.2648e-01, -4.4788e-01, -5.4272e-01,  ...,  6.8109e-01,\n",
       "           -4.1825e-01, -2.9398e-01],\n",
       "          [-1.1707e+00, -4.2548e-01,  2.2624e-01,  ...,  1.0777e-02,\n",
       "            3.4260e-01, -8.4270e-02],\n",
       "          ...,\n",
       "          [ 5.8725e-01, -9.6326e-01, -8.1299e-03,  ...,  2.3240e-01,\n",
       "            1.9140e-01, -4.2964e-01],\n",
       "          [ 7.9164e-01, -1.2455e+00,  4.3706e-01,  ...,  2.8581e-01,\n",
       "           -1.8692e-01, -3.8927e-01],\n",
       "          [ 1.5421e-01, -1.4224e+00,  6.2779e-01,  ...,  5.1895e-01,\n",
       "            5.5302e-02, -1.4453e+00]],\n",
       " \n",
       "         [[ 4.8037e-02, -2.6923e-01, -2.6931e-01,  ...,  3.7911e-01,\n",
       "            1.0444e-01,  1.4585e-01],\n",
       "          [ 8.9621e-01, -6.1535e-01,  3.1738e-03,  ...,  1.4968e-01,\n",
       "            5.9908e-01,  1.6854e-01],\n",
       "          [-1.9184e+00, -9.5781e-01,  3.6379e-01,  ..., -1.1926e-02,\n",
       "            1.1172e+00,  1.6884e+00],\n",
       "          ...,\n",
       "          [ 8.9424e-01, -5.1121e-01, -1.3090e-01,  ...,  2.9590e-01,\n",
       "            8.4242e-01, -1.0117e+00],\n",
       "          [ 1.0514e+00, -8.2156e-01,  4.9424e-01,  ...,  4.0995e-01,\n",
       "            5.6982e-01, -7.8036e-01],\n",
       "          [ 1.4459e-01, -1.2497e+00,  6.3508e-01,  ...,  6.2603e-01,\n",
       "            6.3093e-01, -1.8842e+00]],\n",
       " \n",
       "         [[-8.5167e-02, -4.2012e-01,  1.2017e-01,  ...,  5.8469e-02,\n",
       "           -2.2214e-01,  3.5242e-01],\n",
       "          [-4.7232e-01,  8.4533e-01, -1.9721e-01,  ...,  2.0362e-01,\n",
       "           -5.5946e-01,  5.6370e-01],\n",
       "          [-1.8003e+00, -2.5446e-01,  7.1202e-01,  ...,  7.2505e-01,\n",
       "           -4.3124e-02,  3.2014e-01],\n",
       "          ...,\n",
       "          [ 5.5757e-01, -9.0988e-01,  1.9563e-04,  ...,  3.3911e-01,\n",
       "            1.8428e-01, -4.9297e-01],\n",
       "          [ 7.9210e-01, -1.1969e+00,  4.6697e-01,  ...,  3.9284e-01,\n",
       "           -1.8843e-01, -4.5137e-01],\n",
       "          [ 1.4819e-01, -1.3722e+00,  6.1356e-01,  ...,  6.1040e-01,\n",
       "            6.4668e-02, -1.5134e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]]]),\n",
       " tensor([[[ 3.4178e-02, -7.2357e-01, -2.1546e-01,  ...,  2.0109e-01,\n",
       "            1.9530e-01,  6.8220e-01],\n",
       "          [ 5.0888e-05, -5.2909e-01, -9.6973e-01,  ...,  1.0444e+00,\n",
       "           -1.2519e-02,  3.1049e-02],\n",
       "          [-1.0677e+00, -4.3468e-01,  4.1636e-02,  ...,  4.1430e-01,\n",
       "            5.8661e-01,  3.2956e-01],\n",
       "          ...,\n",
       "          [ 6.9107e-01, -9.3172e-01, -3.5086e-01,  ...,  5.3427e-01,\n",
       "            3.9322e-01, -2.8306e-02],\n",
       "          [ 9.3105e-01, -1.1684e+00,  4.0789e-02,  ...,  4.7272e-01,\n",
       "            1.7595e-01, -5.3663e-02],\n",
       "          [ 4.3487e-01, -1.1880e+00,  4.1473e-01,  ...,  1.0365e+00,\n",
       "            3.2928e-01, -1.1089e+00]],\n",
       " \n",
       "         [[ 1.0170e-01, -8.0950e-01, -5.2200e-01,  ...,  9.2500e-02,\n",
       "            3.3450e-01,  4.8962e-01],\n",
       "          [ 8.0007e-01, -6.3587e-01,  3.5364e-02,  ...,  1.6110e-01,\n",
       "            1.3286e-01,  4.1287e-01],\n",
       "          [-1.6983e+00, -1.1839e+00, -5.0109e-02,  ...,  2.0760e-01,\n",
       "            5.0679e-01,  1.5228e+00],\n",
       "          ...,\n",
       "          [ 1.0642e+00, -2.7302e-01, -3.1365e-01,  ...,  4.5191e-01,\n",
       "            6.1058e-01, -9.6277e-01],\n",
       "          [ 1.2450e+00, -5.9153e-01,  3.0645e-01,  ...,  4.4677e-01,\n",
       "            5.4245e-01, -7.0609e-01],\n",
       "          [ 3.6666e-01, -1.0127e+00,  7.3050e-01,  ...,  1.1186e+00,\n",
       "            6.3555e-01, -1.7294e+00]],\n",
       " \n",
       "         [[ 7.0127e-02, -6.7501e-01, -2.8550e-01,  ...,  1.4452e-01,\n",
       "            1.6228e-01,  6.9303e-01],\n",
       "          [-1.2525e-01,  6.2657e-01, -5.8516e-01,  ...,  6.7510e-01,\n",
       "           -1.3429e-01,  1.0488e+00],\n",
       "          [-1.8574e+00, -3.6162e-01,  4.6865e-01,  ...,  9.9983e-01,\n",
       "            3.1188e-01,  6.1062e-01],\n",
       "          ...,\n",
       "          [ 6.5267e-01, -7.9836e-01, -3.8995e-01,  ...,  6.0053e-01,\n",
       "            3.9210e-01, -1.2024e-01],\n",
       "          [ 9.2077e-01, -1.0552e+00,  4.1444e-02,  ...,  5.3520e-01,\n",
       "            1.6879e-01, -1.4408e-01],\n",
       "          [ 4.3101e-01, -1.0938e+00,  3.4724e-01,  ...,  1.0568e+00,\n",
       "            3.1485e-01, -1.1793e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]]]),\n",
       " tensor([[[ 2.2915e-02, -1.5926e-01, -4.5258e-01,  ...,  8.5378e-01,\n",
       "            1.2012e-01,  4.7014e-01],\n",
       "          [ 3.5046e-01,  1.2516e-03, -1.2891e+00,  ...,  1.2913e+00,\n",
       "            2.8452e-01, -2.3943e-01],\n",
       "          [-8.2092e-01,  2.7921e-01, -1.2979e-01,  ...,  1.2395e+00,\n",
       "            3.5734e-01,  4.8322e-02],\n",
       "          ...,\n",
       "          [ 9.7686e-01, -8.9034e-02, -6.8235e-01,  ...,  8.6007e-01,\n",
       "           -3.2147e-01, -6.1729e-01],\n",
       "          [ 1.2155e+00, -1.9273e-01, -4.3442e-01,  ...,  8.2936e-01,\n",
       "           -3.4010e-01, -6.6638e-01],\n",
       "          [ 7.3137e-01, -1.2691e-02,  1.1332e-01,  ...,  1.3028e+00,\n",
       "           -2.4345e-01, -1.3042e+00]],\n",
       " \n",
       "         [[ 7.5456e-02, -8.2612e-01, -4.3557e-01,  ...,  2.5251e-02,\n",
       "            2.9706e-01,  4.7426e-01],\n",
       "          [ 7.3413e-01, -4.7409e-01,  5.3475e-01,  ...,  2.3984e-01,\n",
       "            5.5776e-01,  4.0490e-01],\n",
       "          [-1.5854e+00, -1.2801e+00, -7.6243e-02,  ...,  4.7753e-01,\n",
       "            1.5818e-01,  1.4282e+00],\n",
       "          ...,\n",
       "          [ 9.2153e-01,  2.9817e-01, -4.5791e-01,  ...,  5.7526e-01,\n",
       "            5.7393e-01, -7.0973e-01],\n",
       "          [ 1.1759e+00, -5.4228e-02,  9.7938e-02,  ...,  3.7070e-01,\n",
       "            5.5054e-01, -4.5505e-01],\n",
       "          [ 3.8849e-01, -1.4354e-01,  6.2060e-01,  ...,  8.9612e-01,\n",
       "            5.5885e-01, -1.5421e+00]],\n",
       " \n",
       "         [[ 5.3806e-02, -2.0401e-01, -4.5596e-01,  ...,  7.6094e-01,\n",
       "            1.1841e-01,  4.7555e-01],\n",
       "          [ 2.6900e-01,  8.1109e-01, -8.1198e-01,  ...,  1.2035e+00,\n",
       "           -4.0135e-02,  7.1379e-01],\n",
       "          [-1.3760e+00,  1.2509e-01,  1.7887e-01,  ...,  1.7582e+00,\n",
       "            2.5414e-01,  4.1315e-01],\n",
       "          ...,\n",
       "          [ 9.0451e-01, -2.2888e-02, -6.2656e-01,  ...,  9.1486e-01,\n",
       "           -3.0002e-01, -7.5414e-01],\n",
       "          [ 1.1864e+00, -1.6422e-01, -3.2539e-01,  ...,  8.8932e-01,\n",
       "           -3.1673e-01, -8.0886e-01],\n",
       "          [ 7.1133e-01, -2.9474e-02,  1.8089e-01,  ...,  1.3782e+00,\n",
       "           -1.7879e-01, -1.3999e+00]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]],\n",
       " \n",
       "         [[        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          ...,\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan],\n",
       "          [        nan,         nan,         nan,  ...,         nan,\n",
       "                   nan,         nan]]]),\n",
       " tensor([[[ 0.4001,  0.8601,  0.0325,  ...,  0.3174,  0.4919,  0.1521],\n",
       "          [ 0.6137,  0.5287, -0.7021,  ...,  0.2867,  0.6652, -0.0560],\n",
       "          [-0.2757,  1.2734,  0.3544,  ...,  0.3800,  0.8228,  0.1945],\n",
       "          ...,\n",
       "          [ 0.8731,  1.0129,  0.1529,  ...,  0.0987,  0.0529, -0.7556],\n",
       "          [ 1.0096,  0.8881,  0.1892,  ...,  0.0716,  0.0098, -0.9534],\n",
       "          [ 0.6824,  1.0828,  0.5332,  ...,  0.5116,  0.2422, -1.3182]],\n",
       " \n",
       "         [[ 0.2657, -0.9541, -0.2121,  ..., -0.0177,  0.6917,  0.1839],\n",
       "          [ 0.7942, -1.0160,  0.5053,  ...,  0.0113,  0.5650,  0.3604],\n",
       "          [-1.4281, -1.4869, -0.2211,  ..., -0.0046,  0.1825,  1.2016],\n",
       "          ...,\n",
       "          [ 0.6027,  0.4202, -0.1336,  ...,  0.4072,  0.3759, -1.0418],\n",
       "          [ 0.8311, -0.0196,  0.2279,  ...,  0.2962,  0.4914, -0.7037],\n",
       "          [-0.0377, -0.1395,  0.7625,  ...,  0.4712,  0.5147, -1.6594]],\n",
       " \n",
       "         [[ 0.3548,  0.7347,  0.0074,  ...,  0.3757,  0.4299,  0.1640],\n",
       "          [ 0.3666,  1.1728, -0.2046,  ...,  0.4740,  0.7201,  0.3826],\n",
       "          [-0.8339,  0.8693,  0.5609,  ...,  1.2747,  0.8284,  0.4103],\n",
       "          ...,\n",
       "          [ 0.8104,  1.0127,  0.1374,  ...,  0.3339, -0.0127, -0.9044],\n",
       "          [ 0.9812,  0.8533,  0.1878,  ...,  0.3282, -0.0544, -1.1043],\n",
       "          [ 0.6419,  0.9866,  0.5564,  ...,  0.7693,  0.2007, -1.4545]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.1557,  0.4111,  1.1208,  ...,  0.3101,  0.2247,  0.0046],\n",
       "          [ 0.2902,  0.1210,  0.5710,  ...,  0.0873,  0.2034, -0.0597],\n",
       "          [-0.4642,  0.7522,  1.1638,  ...,  0.4265,  0.3748,  0.0150],\n",
       "          ...,\n",
       "          [ 0.6591,  0.4749,  0.8605,  ...,  0.3101, -0.2173, -0.8994],\n",
       "          [ 0.7732,  0.3752,  0.9158,  ...,  0.2876, -0.2196, -0.9571],\n",
       "          [ 0.5679,  0.5956,  1.1618,  ...,  0.2967,  0.0327, -1.1664]],\n",
       " \n",
       "         [[ 0.1978, -0.8302,  0.1841,  ...,  0.0393,  0.6744,  0.2576],\n",
       "          [ 0.9978, -1.0719,  0.7184,  ..., -0.1607,  0.4614,  0.2926],\n",
       "          [-1.3715, -1.3674,  0.2973,  ...,  0.1122,  0.0747,  1.1189],\n",
       "          ...,\n",
       "          [ 0.8273,  0.4173, -0.3928,  ...,  0.3503,  0.5171, -1.2253],\n",
       "          [ 1.0278, -0.1558, -0.1088,  ...,  0.1343,  0.8287, -0.6747],\n",
       "          [-0.0118, -0.2366,  0.1025,  ..., -0.2219,  0.7972, -1.9792]],\n",
       " \n",
       "         [[ 0.2431,  0.4246,  0.9181,  ...,  0.2495,  0.3905,  0.0676],\n",
       "          [ 0.1753,  1.1302,  0.7237,  ...,  0.7486,  0.2050,  0.3929],\n",
       "          [-0.7681,  0.5459,  0.9925,  ...,  0.7557,  0.6314,  0.0823],\n",
       "          ...,\n",
       "          [ 0.7283,  0.6466,  0.7204,  ...,  0.4070, -0.0958, -0.9888],\n",
       "          [ 0.8676,  0.5251,  0.7746,  ...,  0.3820, -0.0833, -1.0578],\n",
       "          [ 0.6592,  0.7020,  1.0552,  ...,  0.4503,  0.2073, -1.2380]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.0580,  0.8996,  0.7934,  ..., -0.4680,  0.3037, -0.1799],\n",
       "          [ 0.3535,  0.5130,  0.4091,  ..., -0.4922,  0.5582, -0.2521],\n",
       "          [-0.2678,  0.8697,  0.9774,  ..., -0.0408,  0.5679, -0.2984],\n",
       "          ...,\n",
       "          [ 0.5113,  0.7069,  0.6748,  ..., -0.0650,  0.2116, -0.8881],\n",
       "          [ 0.5520,  0.6356,  0.6534,  ..., -0.0762,  0.2125, -0.8936],\n",
       "          [ 0.4929,  0.8655,  0.6816,  ..., -0.1516,  0.3880, -1.1323]],\n",
       " \n",
       "         [[ 0.3036, -0.4772, -0.2124,  ..., -0.1947,  0.3702,  0.3935],\n",
       "          [ 0.7766, -1.0554,  0.6962,  ..., -0.0261,  0.4849,  0.5090],\n",
       "          [-1.3545, -1.2192,  0.2060,  ..., -0.2751,  0.0610,  0.9382],\n",
       "          ...,\n",
       "          [ 0.6032,  0.1878, -0.1146,  ...,  0.5799,  0.3633, -1.2932],\n",
       "          [ 0.7719, -0.2909,  0.0183,  ...,  0.4175,  0.6614, -0.8094],\n",
       "          [-0.1728, -0.3196, -0.1880,  ..., -0.1177,  0.2092, -1.9722]],\n",
       " \n",
       "         [[ 0.1087,  0.8554,  0.6811,  ..., -0.4909,  0.3891, -0.1155],\n",
       "          [ 0.5667,  0.8697,  0.7443,  ...,  0.0762,  0.3147,  0.1607],\n",
       "          [-0.6430,  0.4435,  1.0015,  ...,  0.2208,  0.4574, -0.1212],\n",
       "          ...,\n",
       "          [ 0.5566,  0.7427,  0.5388,  ...,  0.1482,  0.2478, -0.9298],\n",
       "          [ 0.6352,  0.6619,  0.5148,  ...,  0.1291,  0.2597, -0.9519],\n",
       "          [ 0.5709,  0.8515,  0.5914,  ...,  0.0833,  0.4859, -1.1762]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.0231,  1.3194,  1.1488,  ..., -0.8438,  0.3167,  0.1528],\n",
       "          [ 0.2579,  0.8454,  0.9035,  ..., -0.7751,  0.4739,  0.1809],\n",
       "          [-0.2002,  1.2949,  1.2854,  ..., -0.4134,  0.4395,  0.0942],\n",
       "          ...,\n",
       "          [ 0.4351,  1.1362,  1.0806,  ..., -0.5632,  0.1810, -0.6417],\n",
       "          [ 0.4598,  1.0501,  1.1040,  ..., -0.5756,  0.1922, -0.6982],\n",
       "          [ 0.4024,  1.3402,  0.9824,  ..., -0.7638,  0.4028, -0.8659]],\n",
       " \n",
       "         [[ 0.2939, -0.3903, -0.1279,  ..., -0.0177,  0.1794,  0.2040],\n",
       "          [ 0.7855, -1.1286,  1.0406,  ...,  0.0470,  0.8988,  0.3865],\n",
       "          [-1.1965, -1.2045,  0.2325,  ..., -0.0126,  0.1193,  0.6702],\n",
       "          ...,\n",
       "          [ 0.4155, -0.0440, -0.0477,  ...,  0.2618,  0.2295, -1.4391],\n",
       "          [ 0.5119, -0.5195,  0.0651,  ...,  0.0732,  0.4730, -0.9684],\n",
       "          [-0.0432, -0.5081, -0.2337,  ..., -0.4529, -0.0661, -2.0208]],\n",
       " \n",
       "         [[ 0.0902,  1.3318,  1.2496,  ..., -0.9714,  0.3221,  0.1972],\n",
       "          [ 0.4094,  1.3102,  1.2102,  ..., -0.4259,  0.1942,  0.4240],\n",
       "          [-0.6587,  0.9822,  1.5776,  ..., -0.1963,  0.2868,  0.1871],\n",
       "          ...,\n",
       "          [ 0.5018,  1.2477,  1.1588,  ..., -0.4808,  0.1732, -0.6451],\n",
       "          [ 0.5592,  1.1453,  1.1905,  ..., -0.5015,  0.1910, -0.7128],\n",
       "          [ 0.5264,  1.3995,  1.0866,  ..., -0.6647,  0.4653, -0.8819]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.4577,  1.2497,  0.9615,  ..., -1.2637, -0.0638,  0.0250],\n",
       "          [ 0.5654,  0.9871,  0.9315,  ..., -1.1730, -0.0151,  0.0343],\n",
       "          [ 0.1870,  1.2377,  1.1200,  ..., -0.8913, -0.0544,  0.0332],\n",
       "          ...,\n",
       "          [ 0.7795,  1.0445,  0.9508,  ..., -1.0989, -0.1153, -0.6310],\n",
       "          [ 0.7952,  1.0062,  0.9426,  ..., -1.0862, -0.0925, -0.6741],\n",
       "          [ 0.7667,  1.3065,  0.9035,  ..., -1.2043,  0.0323, -0.7873]],\n",
       " \n",
       "         [[ 0.1263, -0.5969, -0.3776,  ...,  0.3036,  0.0841,  0.1741],\n",
       "          [ 0.4132, -1.6144,  1.2739,  ..., -0.1198,  0.9720, -0.0066],\n",
       "          [-1.4459, -1.8282,  0.3979,  ..., -0.0852,  0.1488,  0.5529],\n",
       "          ...,\n",
       "          [ 0.2231,  0.1231,  0.0855,  ..., -0.0834, -0.1346, -1.9213],\n",
       "          [ 0.3655, -0.3016,  0.0934,  ..., -0.1956,  0.1178, -1.4581],\n",
       "          [-0.1164, -0.1101,  0.0418,  ..., -0.6991, -0.2123, -2.3648]],\n",
       " \n",
       "         [[ 0.4806,  1.1658,  1.1969,  ..., -1.4284,  0.0460,  0.1744],\n",
       "          [ 0.6562,  1.1250,  1.1092,  ..., -1.0726, -0.0832,  0.4390],\n",
       "          [-0.1795,  0.9679,  1.4155,  ..., -0.7430, -0.0684,  0.1615],\n",
       "          ...,\n",
       "          [ 0.8262,  1.0622,  1.1460,  ..., -1.1968, -0.0178, -0.4551],\n",
       "          [ 0.8782,  1.0133,  1.1427,  ..., -1.1966,  0.0100, -0.5011],\n",
       "          [ 0.8735,  1.2984,  1.1319,  ..., -1.2760,  0.1666, -0.6329]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.1976,  0.7523,  0.5583,  ..., -1.6176,  0.0343,  0.0534],\n",
       "          [ 0.2148,  0.6838,  0.5081,  ..., -1.4592, -0.0518,  0.1111],\n",
       "          [-0.1288,  0.7964,  0.6436,  ..., -1.2829,  0.0113,  0.1375],\n",
       "          ...,\n",
       "          [ 0.3712,  0.6819,  0.6411,  ..., -1.5016, -0.1138, -0.5487],\n",
       "          [ 0.3685,  0.6625,  0.6390,  ..., -1.4944, -0.1063, -0.5766],\n",
       "          [ 0.4136,  0.8688,  0.6222,  ..., -1.6005,  0.0346, -0.6944]],\n",
       " \n",
       "         [[ 0.2319, -0.2031, -0.4665,  ...,  0.1604,  0.2482, -0.1150],\n",
       "          [ 0.3739, -1.3886,  1.3132,  ...,  0.0457,  0.9216,  0.2536],\n",
       "          [-1.0351, -1.4574,  0.7594,  ...,  0.3186,  0.3862,  0.5830],\n",
       "          ...,\n",
       "          [ 0.5198,  0.3433,  0.2971,  ..., -0.0478, -0.2104, -2.0832],\n",
       "          [ 0.7237, -0.1427,  0.3751,  ..., -0.1294,  0.0147, -1.5814],\n",
       "          [ 0.2333,  0.3457,  0.4615,  ..., -0.6641, -0.3254, -2.5071]],\n",
       " \n",
       "         [[ 0.2059,  0.6515,  0.8662,  ..., -1.8021,  0.2220,  0.2526],\n",
       "          [ 0.2311,  0.7460,  0.7300,  ..., -1.3559,  0.0778,  0.4275],\n",
       "          [-0.3675,  0.5218,  1.1139,  ..., -1.1332,  0.0603,  0.3450],\n",
       "          ...,\n",
       "          [ 0.3681,  0.6902,  0.9847,  ..., -1.5703,  0.0479, -0.3239],\n",
       "          [ 0.3985,  0.6609,  0.9828,  ..., -1.5680,  0.0633, -0.3573],\n",
       "          [ 0.4699,  0.8418,  0.9699,  ..., -1.6920,  0.2439, -0.4806]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]),\n",
       " tensor([[[ 0.0861,  0.4595, -0.0709,  ..., -1.0305,  0.3560, -0.2889],\n",
       "          [ 0.0403,  0.3982, -0.1082,  ..., -0.9671,  0.3193, -0.2388],\n",
       "          [-0.1936,  0.4635, -0.0174,  ..., -0.9014,  0.4085, -0.2729],\n",
       "          ...,\n",
       "          [ 0.1257,  0.4426, -0.0369,  ..., -0.9885,  0.2781, -0.6403],\n",
       "          [ 0.1201,  0.4386, -0.0380,  ..., -1.0037,  0.2847, -0.6746],\n",
       "          [ 0.1453,  0.5562, -0.0452,  ..., -1.0798,  0.3396, -0.6972]],\n",
       " \n",
       "         [[-0.5357,  0.1796, -0.8137,  ..., -0.3715,  0.8671, -0.2234],\n",
       "          [ 0.3625, -1.2460,  1.3531,  ..., -0.0473,  1.3515,  0.6114],\n",
       "          [-0.8280, -0.7265,  0.6833,  ..., -0.0130,  0.4554,  0.6131],\n",
       "          ...,\n",
       "          [ 0.1689,  0.2625,  0.1807,  ..., -0.5406, -0.2531, -1.3735],\n",
       "          [ 0.4260,  0.1235,  0.3142,  ..., -0.6385, -0.1763, -1.3025],\n",
       "          [ 0.1245,  0.1744,  0.2928,  ..., -0.9077,  0.0663, -1.6904]],\n",
       " \n",
       "         [[ 0.1488,  0.3814,  0.1041,  ..., -0.9066,  0.3909, -0.0765],\n",
       "          [ 0.0541,  0.4333,  0.0544,  ..., -0.8731,  0.4323,  0.0805],\n",
       "          [-0.1159,  0.3610,  0.2696,  ..., -0.6726,  0.4563, -0.1446],\n",
       "          ...,\n",
       "          [ 0.1960,  0.4854,  0.2003,  ..., -0.8808,  0.3736, -0.4551],\n",
       "          [ 0.1965,  0.4779,  0.2008,  ..., -0.8895,  0.3823, -0.4874],\n",
       "          [ 0.2303,  0.5845,  0.2044,  ..., -0.9795,  0.4399, -0.5122]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
       " \n",
       "         [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          ...,\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
       "          [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13\n",
      "Number of batches: 10\n",
      "Number of tokens: 512\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of layers: {len(hidden_states)}')\n",
    "print(f'Number of batches: {len(hidden_states[0])}')\n",
    "print(f'Number of tokens: {len(hidden_states[0][0])}')\n",
    "print(f'Number of hidden units: {len(hidden_states[0][0][0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 768])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_layers = torch.stack(hidden_states, dim=0)[-1]\n",
    "token_layers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512, 13, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Since the dimension is in form [# layers, # batches, # tokens, # features]\n",
    "# # we need it in form [#batches, # tokens, # layers, # features]\n",
    "# token_embedding = token_layers.permute(1, 2, 0, 3)\n",
    "# token_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1050,  0.1815,  0.2432,  ...,  0.5991,  0.0243, -0.7542],\n",
      "         [-0.4050, -0.1798,  0.3960,  ...,  0.4193,  0.2160, -0.8387],\n",
      "         [-0.3172, -0.1309,  0.5166,  ...,  0.2522,  0.2447, -0.8420],\n",
      "         ...,\n",
      "         [-0.3482, -0.1776,  0.4865,  ...,  0.4958,  0.3516, -0.8504],\n",
      "         [-0.3394, -0.1610,  0.5153,  ...,  0.5064,  0.3704, -0.8461],\n",
      "         [-0.2966, -0.1643,  0.5163,  ...,  0.4877,  0.3637, -0.8458]],\n",
      "\n",
      "        [[-0.1877, -0.0602, -0.0828,  ...,  0.4321,  0.2986, -0.7475],\n",
      "         [-0.4315,  0.3184,  0.5249,  ..., -0.6782, -0.5140, -0.4113],\n",
      "         [-0.1051,  0.6852,  0.5233,  ..., -0.5658,  0.1412, -0.0932],\n",
      "         ...,\n",
      "         [-0.1745, -0.2217, -0.5206,  ...,  0.4689,  0.6779, -0.4577],\n",
      "         [-0.1104, -0.1711, -0.3864,  ...,  0.4355,  0.6333, -0.4135],\n",
      "         [-0.4502,  0.0389, -0.6640,  ...,  0.1800,  0.7179, -0.4928]],\n",
      "\n",
      "        [[-0.1121,  0.2323,  0.1242,  ...,  0.4887, -0.0598, -0.6045],\n",
      "         [-0.4099, -0.0529,  0.2986,  ...,  0.3147, -0.0315, -0.7619],\n",
      "         [-0.3769,  0.1214,  0.3698,  ...,  0.0576, -0.0461, -0.7442],\n",
      "         ...,\n",
      "         [-0.2970, -0.0509,  0.4098,  ...,  0.4089,  0.1432, -0.7852],\n",
      "         [-0.2956, -0.0394,  0.4458,  ...,  0.4250,  0.1711, -0.7756],\n",
      "         [-0.2421, -0.0520,  0.4398,  ...,  0.4016,  0.1783, -0.7738]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]],\n",
      "\n",
      "        [[    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         ...,\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan],\n",
      "         [    nan,     nan,     nan,  ...,     nan,     nan,     nan]]],\n",
      "       grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Cut the layer\n",
    "layer = nn.RNN(input_size=768, hidden_size=512, batch_first=True)\n",
    "out, _ = layer(token_layers)\n",
    "\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
