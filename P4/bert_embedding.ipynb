{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tryout bert embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "# import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.', 1)\n"
     ]
    }
   ],
   "source": [
    "# combine both negative and pos in train and test to train\n",
    "dataSet = {}\n",
    "dataSet['train'] = []\n",
    "dataSet['test'] = []\n",
    "for p in Path('data/text_classification/train/pos/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['train'].append((entry, 1))\n",
    "for p in Path('data/text_classification/train/neg/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['train'].append((entry, 0))\n",
    "\n",
    "for p in Path('data/text_classification/test/pos/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['test'].append((entry, 1))\n",
    "for p in Path('data/text_classification/test/neg/').glob('*.txt'): # 1 is pos and 0 is neg\n",
    "    entry = p.read_text(encoding='utf8')\n",
    "    dataSet['test'].append((entry, 0))\n",
    "\n",
    "\n",
    "print(dataSet['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# with bert we don't need to split the sentences. But do remove the html\n",
    "cleanedTrainData = []\n",
    "CLEANHTML = re.compile('<.*?>')\n",
    "\n",
    "for entry in dataSet['train']:\n",
    "    # ! Hope the data doesn't contain heavy html tags or else it wouldn't work\n",
    "    text = re.sub(CLEANHTML, '', entry[0])\n",
    "    cleanedTrainData.append((text, entry[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.\n",
      "When this movie first came out back in 1984, Prince was one of the hottest acts around. Everyone wanted to see this movie, which was not much more than a extended music video. The acting was pretty bad, but what can you expect from musicians acting on the big screen for the first time? Despite that, it was still a very entertaining film! Morris Day and Jerome Benton provide some all time classic comedy, especially their rendition of \"The Password\", which will make you think of Abbott & Costello doing their \"who's on first\" baseball routine.Appolina (who went by a single name then) provided some beautiful breasts, so you had the brief nudity covered. Plus, she is very attractive. And of course, the soundtrack of the album is one of the best Prince ever recorded. Prince later on had a fallout with Warner Bros. and changed his name, but at this particular time in his career, he was at the top of his game.This movie doesn't rank in the all time great category, but it is pretty entertaining.\n",
      "I watched the Pie-lette last night and the word that comes to mind is \"original.\" It is a word not used much in TV as they all tend to copy whatever the other network is doing and you end up with seven nights of crime shows, unfunny comedies, and reality crap.The first thing that hit me like a brick was the presence of Jim Dale. Those not familiar with the British \"Carry On ...\" series or those who have not listened to a Harry Potter book, may not be familiar with Dale. I am not sure whether his presence as narrator adds or distracts. I will have to tune in more, but it does give the show a \"Harry Potter\" atmosphere. Maybe that's a good thing.Lee Pace (Infamous, The White Countess) has a gift. It never explains where he got it, but he can bring someone back from the dead for a minute. He teams with Chi McBride (\"Boston Public,\" Roll Bounce) to solve murders using this talent. Everything is fine and funny until he comes across a childhood love, Anna Friel (Goal! The Dream Begins, Timeline) and things really get complicated. He can't send her back and he can never touch her. Boy, would that make a relationship difficult.I will be tuning in to see where this series goes in the expectation that it will continue to entertain.\n"
     ]
    }
   ],
   "source": [
    "# try print out a cleaned text\n",
    "for i in range(0, 3):\n",
    "    print(cleanedTrainData[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 22:57:47.166878: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 22:57:47.345234: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-27 22:57:47.385659: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-27 22:57:48.066906: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-27 22:57:48.067029: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-27 22:57:48.067036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
      "[2478, 1037, 10938, 2121, 2897, 2003, 3722]\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1458 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max sentences length is: 1458\n"
     ]
    }
   ],
   "source": [
    "# Add special required formatting for BERT\n",
    "# maxSentenceLen = 0\n",
    "# for text, label in cleanedTrainData:\n",
    "#     input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "# \n",
    "#     maxSentenceLen = max(maxSentenceLen, len(input_ids))\n",
    "# \n",
    "# print(f'The max sentences length is: {maxSentenceLen}')\n",
    "\n",
    "# ! BERT only take 512 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "sent = 'I hate this. So much'\n",
    "tokened = tokenizer(sent, padding='max_lenght', max_length=512, add_special_tokens=True)\n",
    "done = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# test truncation methods [cut at the end -> shown to give better result]\n",
    "# ? https://stackoverflow.com/questions/58636587/how-to-use-bert-for-long-text-classification?noredirect=1&lq=1\n",
    "# ? https://github.com/huggingface/transformers/issues/4476#issuecomment-951445067\n",
    "def encode_right_truncated(tokenizer, text, padding='max_length', max_length=512, add_special_tokens=True):\n",
    "    out = tokenizer(text, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "    tokenized = out['input_ids']\n",
    "    print(f\"Length of the inputID: {len(tokenized)}, length of token type: {len(out['token_type_ids'])}, length of mask: {len(out['attention_mask'])}\")\n",
    "    print(f\"type: {out['token_type_ids']}\")\n",
    "    print(f\"type: {out['attention_mask']}\")\n",
    "    if not add_special_tokens:\n",
    "        truncated = tokenized[-max_length:]\n",
    "    else:\n",
    "        truncated = tokenized[0:1] + tokenized[-(max_length-1):]    # keep special start and end symbol\n",
    "    \n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the inputID: 512, length of token type: 512, length of mask: 512\n",
      "type: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "type: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 6551, 5632, 2023, 3386, 6547, 10874, 2143, 2055, 1037, 2402, 2450, 1010, 9401, 1042, 11663, 1010, 1006, 6423, 5811, 1007, 2040, 2003, 2041, 1997, 2147, 1998, 2038, 5357, 2369, 1999, 2014, 9278, 1998, 2003, 7143, 2000, 2424, 2147, 1012, 6423, 9631, 2019, 4748, 1999, 1996, 2334, 2414, 3780, 2559, 2005, 1037, 3187, 1998, 18545, 2041, 2000, 3046, 1998, 6855, 2023, 2597, 1012, 6423, 6855, 2015, 1996, 2597, 1998, 2003, 5086, 2011, 1037, 3680, 1012, 8099, 1010, 1006, 8214, 2089, 25591, 1007, 2040, 5942, 2008, 2016, 3268, 2007, 2014, 11194, 1999, 2014, 2188, 1998, 4122, 2014, 2000, 2031, 2053, 6624, 2007, 2273, 2814, 1998, 6423, 4136, 2068, 2016, 2038, 2053, 2155, 1998, 2003, 2489, 2000, 23313, 2014, 2972, 2051, 2000, 2023, 3105, 1012, 2577, 6097, 16416, 5149, 1010, 1006, 6798, 8099, 1007, 2003, 1996, 2365, 1997, 3680, 1012, 8099, 1998, 2038, 2070, 2200, 4326, 14714, 2005, 2652, 2105, 2007, 13227, 1012, 2023, 2001, 1037, 2659, 5166, 2143, 1998, 2087, 1997, 1996, 5019, 2020, 2485, 11139, 1999, 2344, 2000, 4468, 1996, 10961, 1997, 1037, 4281, 1998, 5366, 2005, 17363, 1012, 2023, 4326, 2155, 2035, 2444, 1999, 1037, 4121, 7330, 2125, 1996, 10387, 3023, 1997, 2563, 1998, 2045, 2003, 3595, 4303, 1998, 7564, 1997, 23873, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Keeping only 512 word at the end\n",
    "count = 0\n",
    "\n",
    "for text, label in cleanedTrainData:\n",
    "    input_ids = encode_right_truncated(tokenizer, text)\n",
    "    print(input_ids)\n",
    "    break\n",
    "# print(f'The max sentences length is: {maxSentenceLen}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# unpacking\n",
    "wow = {\n",
    "    'a': 1,\n",
    "    'b': 20\n",
    "}\n",
    "\n",
    "def func (a, b):\n",
    "    print(a, b)\n",
    "\n",
    "# func(**{'a': 1, 'b': 2}, **{'c':4, 'd':5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# LOOKING GOOD, now we can reuse the previous loop to create the input tensor\n",
    "\n",
    "def encode_right_truncated(tokenizer, text, padding='max_length', max_length=512, add_special_tokens=True):\n",
    "    out = tokenizer(text, padding=padding, max_length=max_length, add_special_tokens=add_special_tokens)\n",
    "    tokenized = out['input_ids']\n",
    "    tokenType = out['token_type_ids']\n",
    "    attention = out['attention_mask']\n",
    "\n",
    "    if not add_special_tokens:\n",
    "        truncated = {\n",
    "            'input_ids': tokenized[-max_length:],\n",
    "            'token_type_ids': tokenType[-max_length:],\n",
    "            'attention_mask': attention[-max_length:],\n",
    "        }\n",
    "    else:\n",
    "        truncated = {\n",
    "            'input_ids': tokenized[0:1] + tokenized[-(max_length-1):],    # keep special start and end symbol\n",
    "            'token_type_ids': tokenType[0:1] + tokenType[-(max_length-1):],\n",
    "            'attention_mask': attention[0:1] + attention[-(max_length-1):],\n",
    "        }\n",
    "    \n",
    "    return truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of inputID: torch.Size([512]) and shape of attention: torch.Size([512]) and label shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "testText = cleanedTrainData[0][0]\n",
    "test = encode_right_truncated(tokenizer, testText)\n",
    "\n",
    "inputIDTensor = torch.tensor(test['input_ids'])\n",
    "attentionTensor = torch.tensor(test['attention_mask'])\n",
    "labelTensor = torch.tensor(cleanedTrainData[0][1])\n",
    "\n",
    "print(f'shape of inputID: {inputIDTensor.shape} and shape of attention: {attentionTensor.shape} and label shape: {labelTensor.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input of RNN should be tensor of (batch size, max length for the whole text, dimesion of the vocab vector [the size of the vector use to represent the vocab]) \n",
    "### in Bert, the shape is (batch, 512, 768) -> 768 is the size of each vocab vector in BERT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
