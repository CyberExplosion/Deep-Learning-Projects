{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What did the bartender say to the jumper cable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't you hate jokes about German sausage? The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Two artists had an art contest... It ended in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Why did the chicken cross the playground? To g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What gun do you use to hunt a moose? A moosecut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>If life gives you melons, you might have dysle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Broken pencils... ...are pointless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What did one snowman say to the other snowman?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How many hipsters does it take to change a lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Where do sick boats go? The dock!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  What did the bartender say to the jumper cable...\n",
       "1   2  Don't you hate jokes about German sausage? The...\n",
       "2   3  Two artists had an art contest... It ended in ...\n",
       "3   4  Why did the chicken cross the playground? To g...\n",
       "4   5   What gun do you use to hunt a moose? A moosecut!\n",
       "5   6  If life gives you melons, you might have dysle...\n",
       "6   7                Broken pencils... ...are pointless.\n",
       "7   8  What did one snowman say to the other snowman?...\n",
       "8   9  How many hipsters does it take to change a lig...\n",
       "9  10                  Where do sick boats go? The dock!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputText = pd.read_csv('data/text_generation/data')\n",
    "inputText.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       What did the bartender say to the jumper cable...\n",
      "1       Don't you hate jokes about German sausage? The...\n",
      "2       Two artists had an art contest... It ended in ...\n",
      "3       Why did the chicken cross the playground? To g...\n",
      "4        What gun do you use to hunt a moose? A moosecut!\n",
      "                              ...                        \n",
      "1617    What do you call a camel with 3 humps? Humphre...\n",
      "1618    Two fish in a tank. [x-post from r/Jokes] One ...\n",
      "1619            \"Stay strong!\" I said to my wi-fi signal.\n",
      "1620    Why was the tomato blushing? Because it saw th...\n",
      "1621      What is heavy forward but not backward? **ton**\n",
      "Name: Joke, Length: 1622, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(inputText.loc[:,'Joke'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokeList = []\n",
    "for each in inputText.loc[:, 'Joke']:\n",
    "    jokeList.append(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from glob import glob\n",
    "import os\n",
    "import unicodedata\n",
    "import string\n",
    "import torch\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all the words in each joke\n",
    "# remove punctuation and all that\n",
    "cleanedJokes = []\n",
    "CLEANHTML = re.compile('<.*?>')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "for joke in jokeList:\n",
    "    text = re.sub(CLEANHTML, '', joke)\n",
    "    # split into white space\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    # remove symbol and stop words\n",
    "    wordList = [word.lower() for word in wordList if word.isalpha() and word not in stopWords]\n",
    "    wordList.append('<EOS>')\n",
    "    cleanedJokes.append(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4330"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build from vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "vocabs = build_vocab_from_iterator(cleanedJokes, specials=['<UNK>'])\n",
    "vocabs.set_default_index(vocabs['<UNK>'])\n",
    "len(vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 198, 7, 2976, 2063, 21, 259, 543, 959, 223, 1]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexesFirstJoke = vocabs(cleanedJokes[0])\n",
    "indexesFirstJoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4330\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocabs)\n",
    "all_words = vocabs.get_itos()\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 198, 7, 2976, 2063, 21, 259, 543, 959, 223, 1]\n",
      "actual word represent\n",
      "['what', 'bartender', 'say', 'jumper', 'cables', 'you', 'better', 'try', 'start', 'anything', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "# Tokenized joke list\n",
    "tokenizedJokes = []\n",
    "for joke in cleanedJokes:\n",
    "    tokenized = vocabs(joke)\n",
    "    tokenizedJokes.append(tokenized)\n",
    "\n",
    "print(tokenizedJokes[0])\n",
    "print('actual word represent')\n",
    "print(cleanedJokes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total of sequences: 9616\n",
      "['what bartender say jumper', 'bartender say jumper cables', 'say jumper cables you', 'jumper cables you better', 'cables you better try', 'you better try start', 'better try start anything', 'do hate jokes german', 'hate jokes german sausage', 'jokes german sausage they', 'german sausage they wurst', 'two artists art contest', 'artists art contest it', 'art contest it ended', 'contest it ended draw', 'why chicken cross playground', 'chicken cross playground to', 'cross playground to get', 'playground to get slide', 'what gun use hunt', 'gun use hunt moose', 'use hunt moose a', 'hunt moose a moosecut', 'if life gives melons', 'life gives melons might', 'gives melons might dyslexia', 'what one snowman say', 'one snowman say snowman', 'snowman say snowman smell', 'say snowman smell carrots', 'how many hipsters take', 'many hipsters take change', 'hipsters take change lightbulb', 'take change lightbulb it', 'change lightbulb it really', 'lightbulb it really obscure', 'it really obscure number', 'really obscure number you', 'obscure number you probably', 'number you probably never', 'you probably never heard', 'where sick boats go', 'sick boats go the', 'boats go the dock', 'i like slaves like', 'like slaves like i', 'slaves like i like', 'like i like coffee', 'i like coffee free', 'my girlfriend told leaving']\n"
     ]
    }
   ],
   "source": [
    "# ! Sequence text into input and output. Ex: 50 input and 1 output\n",
    "# Lest go with 3 input words and 1 output\n",
    "length = 3 + 1\n",
    "sequences = []\n",
    "for joke in cleanedJokes:\n",
    "    for i in range(length, len(joke)):\n",
    "        # select sequence of token\n",
    "        seq = joke[i-length:i]\n",
    "                \n",
    "        # convert to a line\n",
    "        line = ' '.join(seq)\n",
    "        # store\n",
    "        sequences.append(line)\n",
    "\n",
    "print(f\"total of sequences: {len(sequences)}\")\n",
    "print(sequences[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sequences for later\n",
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# save sequences to file\n",
    "out_filename = 'saved/test_sequences.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 198, 7, 2976], [198, 7, 2976, 2063], [7, 2976, 2063, 21]]\n",
      "['what bartender say jumper', 'bartender say jumper cables', 'say jumper cables you']\n"
     ]
    }
   ],
   "source": [
    "# When load  back, tokenized every sequence\n",
    "tokenizedSequences = []\n",
    "for each in sequences:\n",
    "    wordList = nltk.word_tokenize(each)\n",
    "    tokenized = vocabs(wordList)\n",
    "    tokenizedSequences.append(tokenized)\n",
    "\n",
    "print(tokenizedSequences[0:3])\n",
    "print(sequences[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of X: 9616\n",
      "Shape of X: torch.Size([9616, 3]) and of y: torch.Size([9616, 4330])\n"
     ]
    }
   ],
   "source": [
    "# One hot encode the next word as output\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for each in tokenizedSequences:\n",
    "    if len(each) == 4:\n",
    "        X.append(each[:-1])\n",
    "        # Make y one hot encode\n",
    "        y.append(each[-1])\n",
    "    else:\n",
    "        print(f'The tokenized: {each}')\n",
    "        sentence = []\n",
    "        for word in each:\n",
    "            sentence.append(vocabs.lookup_token(word))\n",
    "        print(f'The represetntation is: {sentence}')\n",
    "\n",
    "tensorY = torch.nn.functional.one_hot(torch.tensor(y), num_classes=vocab_size)\n",
    "\n",
    "print(f'Len of X: {len(X)}')\n",
    "tensorX = torch.tensor(X)\n",
    "seq_len = tensorX.shape[1]\n",
    "print(f'Shape of X: {tensorX.shape} and of y: {tensorY.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9616, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0002, 0.0002, 0.0002,  ..., 0.0003, 0.0003, 0.0003],\n",
       "        [0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
       "        [0.0002, 0.0002, 0.0003,  ..., 0.0003, 0.0002, 0.0002],\n",
       "        ...,\n",
       "        [0.0002, 0.0002, 0.0002,  ..., 0.0003, 0.0002, 0.0003],\n",
       "        [0.0002, 0.0002, 0.0002,  ..., 0.0003, 0.0002, 0.0002],\n",
       "        [0.0002, 0.0001, 0.0003,  ..., 0.0002, 0.0002, 0.0003]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: converting the keras embedding into pytorch\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "# model.add(LSTM(100, return_sequences=True))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(100, activation='relu'))\n",
    "# model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "class extract_tensor(nn.Module):\n",
    "    def forward(self,x):\n",
    "        # Output shape (batch, features, hidden)\n",
    "        tensor, _ = x\n",
    "        # Reshape shape (batch, hidden)\n",
    "        ten = tensor[:, -1, :]\n",
    "        print(ten.shape)\n",
    "        flat = nn.Flatten()\n",
    "        ten = flat(ten)\n",
    "        return ten\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=9616, embedding_dim=4330),\n",
    "    nn.LSTM(input_size=vocab_size, hidden_size=100, batch_first=True),\n",
    "    extract_tensor(),\n",
    "\n",
    "    nn.Linear(in_features=100, out_features=vocab_size),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "# summary(model, input_size=(1, 9616, 100))\n",
    "model(tensorX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 1622\n"
     ]
    }
   ],
   "source": [
    "# USELESS\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "# Build the category_lines dict, a list of lines per category\n",
    "category_lines = {}\n",
    "all_categories = []\n",
    "for category, joke in enumerate(cleanedJokes):\n",
    "    all_categories.append(category)\n",
    "    category_lines[category] = joke\n",
    "\n",
    "num_categories = len(all_categories)\n",
    "\n",
    "print(f'Number of categories: {num_categories}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def randChoice(li: list):\n",
    "    return li[random.randint(0, len(li) - 1)]\n",
    "\n",
    "# Get a random pair of category (ID) and a word from that joke category\n",
    "def randomTrainingPair():\n",
    "    category = randChoice(all_categories)\n",
    "    wordList = category_lines[category].split(' ')\n",
    "    line = randChoice(wordList)\n",
    "    return category, line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot vector for category\n",
    "def categoryTensor(category):\n",
    "    li = all_categories.index(category)\n",
    "    tensor = torch.zeros(1, num_categories)\n",
    "    tensor[0][li] = 1\n",
    "    return tensor\n",
    "\n",
    "# One-hot matrix of first to last words (not including EOS) for input\n",
    "def inputTensor(cleanedJokeEntry):\n",
    "    tensor = torch.zeros(len(cleanedJokeEntry), 1, n_words)\n",
    "    for li in range(len(cleanedJokeEntry)):\n",
    "        word = cleanedJokeEntry[li]\n",
    "        print(f'The word: {word}')\n",
    "        tensor[li][0][all_words.index(word)] = 1\n",
    "\n",
    "        print(tensor)\n",
    "        break\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# LongTensor of second letter to end (EOS) for target\n",
    "def targetTensor(line):\n",
    "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
    "    letter_indexes.append(n_letters - 1) # EOS\n",
    "    return torch.LongTensor(letter_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = categoryTensor(1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word: What\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "lol = inputTensor(cleanedJokes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550 ==== to\n"
     ]
    }
   ],
   "source": [
    "category, line = randomTrainingPair()\n",
    "print(category, '====', line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
