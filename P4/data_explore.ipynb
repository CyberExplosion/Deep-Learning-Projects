{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12077_8.txt: \n",
      "Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading in 1 file\n",
    "for p in Path('data/text_classification/train/pos/').glob('*.txt'):\n",
    "    print(f'{p.name}: \\n{p.read_text()}\\n')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.', 'When this movie first came out back in 1984, Prince was one of the hottest acts around. Everyone wanted to see this movie, which was not much more than a extended music video. The acting was pretty bad, but what can you expect from musicians acting on the big screen for the first time? Despite that, it was still a very entertaining film! Morris Day and Jerome Benton provide some all time classic comedy, especially their rendition of \"The Password\", which will make you think of Abbott & Costello doing their \"who\\'s on first\" baseball routine.<br /><br />Appolina (who went by a single name then) provided some beautiful breasts, so you had the brief nudity covered. Plus, she is very attractive. And of course, the soundtrack of the album is one of the best Prince ever recorded. Prince later on had a fallout with Warner Bros. and changed his name, but at this particular time in his career, he was at the top of his game.<br /><br />This movie doesn\\'t rank in the all time great category, but it is pretty entertaining.']\n"
     ]
    }
   ],
   "source": [
    "# Read all files into an array of string\n",
    "posReviews = []\n",
    "for p in Path('data/text_classification/train/pos/').glob('*.txt'):\n",
    "    posReviews.append(p.read_text(encoding='utf8'))\n",
    "print(posReviews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import all text and use the points as their labels\n",
    "# reviews = []\n",
    "# pts = []\n",
    "# count = 0\n",
    "# for p in Path('data/text_classification/train/pos/').glob('*.txt'):\n",
    "#     reviews.append(p.read_text(encoding='utf8'))\n",
    "#     tailName = p.name.split('_')\n",
    "#     grade = tailName[1].strip('.txt')   # strip the tail\n",
    "#     pts.append(grade)\n",
    "#     if count == 3:\n",
    "#         break\n",
    "#     print(f'FIle name: {p.name} File content: {reviews[count]} and grade: {pts[count]}')\n",
    "#     count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # seperate into train and test, positive and negative\n",
    "# reviews = {}\n",
    "# reviews['train'] = {}\n",
    "# reviews['train']['pos'] = []\n",
    "# reviews['train']['neg'] = []\n",
    "# reviews['test'] = {}\n",
    "# reviews['test']['pos'] = []\n",
    "# reviews['test']['neg'] = []\n",
    "# for p in Path('data/text_classification/train/pos/').glob('*.txt'):\n",
    "#     reviews['train']['pos'].append(p.read_text(encoding='utf8'))\n",
    "# for p in Path('data/text_classification/train/neg/').glob('*.txt'):\n",
    "#     reviews['train']['neg'].append(p.read_text(encoding='utf8'))\n",
    "# \n",
    "# for p in Path('data/text_classification/test/pos/').glob('*.txt'):\n",
    "#     reviews['test']['pos'].append(p.read_text(encoding='utf8'))\n",
    "# for p in Path('data/text_classification/test/neg/').glob('*.txt'):\n",
    "#     reviews['test']['neg'].append(p.read_text(encoding='utf8'))\n",
    "# \n",
    "# reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataSet['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shuffle all data and pick to get a good distribution amount of pos and neg review while train\n",
    "# permutation = torch.randperm(len(dataSet['train']))\n",
    "# for i in range(0, 3):\n",
    "#     randIdx = permutation[i]\n",
    "#     print(dataSet['train'][randIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ubuntu/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "# https://stackoverflow.com/questions/9662346/python-code-to-remove-html-tags-from-a-string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have to read them in as dictionary so that we can split each word and turn whole doc to list of word\n",
    "# combine both negative and pos in train and test to train\n",
    "cleanedTrainData = []\n",
    "stopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "CLEANHTML = re.compile('<.*?>')\n",
    "\n",
    "for entry in dataSet['train']:\n",
    "    # ! Hope the data doesn't contain heavy html tags or else it wouldn't work\n",
    "    text = re.sub(CLEANHTML, '', entry[0])\n",
    "    # split into white space\n",
    "    wordList = nltk.word_tokenize(text)\n",
    "    # remove symbol and stop words\n",
    "    wordList = [word for word in wordList if word.isalpha() and word not in stopWords]\n",
    "    # maybe not stem since small amount of documents\n",
    "    # stemmed = [porter.stem(word) for word in wordList]\n",
    "\n",
    "    # print(wordList)\n",
    "    # print(stemmed)\n",
    "\n",
    "    cleanedTrainData.append((wordList, entry[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['Greatly', 'enjoyed', 'mystery', 'thriller', 'film', 'young', 'woman', 'Nina', 'Foch', 'Julia', 'Ross', 'work', 'fallen', 'behind', 'rent', 'desperate', 'find', 'work', 'Julia', 'reads', 'ad', 'local', 'London', 'newspaper', 'looking', 'secretary', 'rushes', 'try', 'obtain', 'position', 'Julia', 'obtains', 'position', 'hired', 'Hughes', 'Dame', 'May', 'Witty', 'requires', 'lives', 'employer', 'home', 'wants', 'involvement', 'men', 'friends', 'Julia', 'tells', 'family', 'free', 'devote', 'entire', 'time', 'job', 'George', 'Macready', 'Ralph', 'Hughes', 'son', 'Hughes', 'strange', 'desires', 'playing', 'around', 'knives', 'This', 'low', 'budget', 'film', 'scenes', 'close', 'ups', 'order', 'avoid', 'expense', 'background', 'costs', 'scenery', 'This', 'strange', 'family', 'live', 'huge', 'mansion', 'Cornwall', 'Coast', 'England', 'secret', 'doors', 'plenty', 'suspense'], 1), (['When', 'movie', 'first', 'came', 'back', 'Prince', 'one', 'hottest', 'acts', 'around', 'Everyone', 'wanted', 'see', 'movie', 'much', 'extended', 'music', 'video', 'The', 'acting', 'pretty', 'bad', 'expect', 'musicians', 'acting', 'big', 'screen', 'first', 'time', 'Despite', 'still', 'entertaining', 'film', 'Morris', 'Day', 'Jerome', 'Benton', 'provide', 'time', 'classic', 'comedy', 'especially', 'rendition', 'The', 'Password', 'make', 'think', 'Abbott', 'Costello', 'first', 'baseball', 'went', 'single', 'name', 'provided', 'beautiful', 'breasts', 'brief', 'nudity', 'covered', 'Plus', 'attractive', 'And', 'course', 'soundtrack', 'album', 'one', 'best', 'Prince', 'ever', 'recorded', 'Prince', 'later', 'fallout', 'Warner', 'changed', 'name', 'particular', 'time', 'career', 'top', 'movie', 'rank', 'time', 'great', 'category', 'pretty', 'entertaining'], 1), (['I', 'watched', 'last', 'night', 'word', 'comes', 'mind', 'original', 'It', 'word', 'used', 'much', 'TV', 'tend', 'copy', 'whatever', 'network', 'end', 'seven', 'nights', 'crime', 'shows', 'unfunny', 'comedies', 'reality', 'first', 'thing', 'hit', 'like', 'brick', 'presence', 'Jim', 'Dale', 'Those', 'familiar', 'British', 'Carry', 'On', 'series', 'listened', 'Harry', 'Potter', 'book', 'may', 'familiar', 'Dale', 'I', 'sure', 'whether', 'presence', 'narrator', 'adds', 'distracts', 'I', 'tune', 'give', 'show', 'Harry', 'Potter', 'atmosphere', 'Maybe', 'good', 'Pace', 'Infamous', 'The', 'White', 'Countess', 'gift', 'It', 'never', 'explains', 'got', 'bring', 'someone', 'back', 'dead', 'minute', 'He', 'teams', 'Chi', 'McBride', 'Boston', 'Public', 'Roll', 'Bounce', 'solve', 'murders', 'using', 'talent', 'Everything', 'fine', 'funny', 'comes', 'across', 'childhood', 'love', 'Anna', 'Friel', 'Goal', 'The', 'Dream', 'Begins', 'Timeline', 'things', 'really', 'get', 'complicated', 'He', 'ca', 'send', 'back', 'never', 'touch', 'Boy', 'would', 'make', 'relationship', 'tuning', 'see', 'series', 'goes', 'expectation', 'continue', 'entertain'], 1)]\n"
     ]
    }
   ],
   "source": [
    "print(cleanedTrainData[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Greatly enjoyed this 1945 mystery thriller film about a young woman, Nina Foch,(Julia Ross) who is out of work and has fallen behind in her rent and is desperate to find work. Julia reads an ad in the local London newspaper looking for a secretary and rushes out to try and obtain this position. Julia obtains the position and is hired by a Mrs. Hughes, (Dame May Witty) who requires that she lives with her employer in her home and wants her to have no involvement with men friends and Julia tells them she has no family and is free to devote her entire time to this job. George Macready, (Ralph Hughes) is the son of Mrs. Hughes and has some very strange desires for playing around with knives. This was a low budget film and most of the scenes were close ups in order to avoid the expense of a background and costs for scenery. This strange family all live in a huge mansion off the Cornwall Coast of England and there is secret doors and plenty of suspense.', 1), ('When this movie first came out back in 1984, Prince was one of the hottest acts around. Everyone wanted to see this movie, which was not much more than a extended music video. The acting was pretty bad, but what can you expect from musicians acting on the big screen for the first time? Despite that, it was still a very entertaining film! Morris Day and Jerome Benton provide some all time classic comedy, especially their rendition of \"The Password\", which will make you think of Abbott & Costello doing their \"who\\'s on first\" baseball routine.<br /><br />Appolina (who went by a single name then) provided some beautiful breasts, so you had the brief nudity covered. Plus, she is very attractive. And of course, the soundtrack of the album is one of the best Prince ever recorded. Prince later on had a fallout with Warner Bros. and changed his name, but at this particular time in his career, he was at the top of his game.<br /><br />This movie doesn\\'t rank in the all time great category, but it is pretty entertaining.', 1), ('I watched the Pie-lette last night and the word that comes to mind is \"original.\" It is a word not used much in TV as they all tend to copy whatever the other network is doing and you end up with seven nights of crime shows, unfunny comedies, and reality crap.<br /><br />The first thing that hit me like a brick was the presence of Jim Dale. Those not familiar with the British \"Carry On ...\" series or those who have not listened to a Harry Potter book, may not be familiar with Dale. I am not sure whether his presence as narrator adds or distracts. I will have to tune in more, but it does give the show a \"Harry Potter\" atmosphere. Maybe that\\'s a good thing.<br /><br />Lee Pace (Infamous, The White Countess) has a gift. It never explains where he got it, but he can bring someone back from the dead for a minute. He teams with Chi McBride (\"Boston Public,\" Roll Bounce) to solve murders using this talent. Everything is fine and funny until he comes across a childhood love, Anna Friel (Goal! The Dream Begins, Timeline) and things really get complicated. He can\\'t send her back and he can never touch her. Boy, would that make a relationship difficult.<br /><br />I will be tuning in to see where this series goes in the expectation that it will continue to entertain.', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(dataSet['train'][0:3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading sentence into BERT embedding \n",
    "Even thought BERT performs better with minimal cleaning. We still getting rid of stop words and punctuation since our task is easy (and I don't want to do the data loading part again on another notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-27 22:29:57.240816: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-27 22:29:57.398852: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-27 22:29:57.439289: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-02-27 22:29:58.070400: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-27 22:29:58.070523: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/amazon/efa/lib:/opt/amazon/openmpi/lib:/usr/local/cuda/efa/lib:/usr/local/cuda/lib:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/targets/x86_64-linux/lib:/usr/local/lib:/usr/lib:\n",
      "2023-02-27 22:29:58.070531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1458 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max sentences length is: 1458\n"
     ]
    }
   ],
   "source": [
    "# Add special required formatting for BERT\n",
    "# maxSentenceLen = 0\n",
    "# for text, label in cleanedTrainData:\n",
    "#     input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "# \n",
    "#     maxSentenceLen = max(maxSentenceLen, len(input_ids))\n",
    "# \n",
    "# print(f'The max sentences length is: {maxSentenceLen}')\n",
    "\n",
    "# ! BERT only take 512 max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only 512 word at the end\n",
    "for text, label in cleanedTrainData:\n",
    "    if len(text > )\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "\n",
    "    maxSentenceLen = max(maxSentenceLen, len(input_ids))\n",
    "\n",
    "print(f'The max sentences length is: {maxSentenceLen}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing name into tensors\n",
    "https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90172"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textNoLabels = [x[0] for x in cleanedTrainData]\n",
    "vocab = build_vocab_from_iterator(textNoLabels, specials=['<UNK>'])\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45055,\n",
       " 376,\n",
       " 777,\n",
       " 648,\n",
       " 3,\n",
       " 98,\n",
       " 146,\n",
       " 6204,\n",
       " 9738,\n",
       " 2534,\n",
       " 5239,\n",
       " 70,\n",
       " 2905,\n",
       " 398,\n",
       " 819,\n",
       " 1618,\n",
       " 71,\n",
       " 70,\n",
       " 2534,\n",
       " 4358,\n",
       " 3676,\n",
       " 557,\n",
       " 1075,\n",
       " 3837,\n",
       " 187,\n",
       " 3573,\n",
       " 9682,\n",
       " 238,\n",
       " 5885,\n",
       " 2552,\n",
       " 2534,\n",
       " 28266,\n",
       " 2552,\n",
       " 2474,\n",
       " 5583,\n",
       " 10390,\n",
       " 2232,\n",
       " 13044,\n",
       " 3263,\n",
       " 345,\n",
       " 10775,\n",
       " 257,\n",
       " 355,\n",
       " 3774,\n",
       " 265,\n",
       " 256,\n",
       " 2534,\n",
       " 548,\n",
       " 124,\n",
       " 835,\n",
       " 18637,\n",
       " 304,\n",
       " 11,\n",
       " 182,\n",
       " 587,\n",
       " 17512,\n",
       " 3024,\n",
       " 5583,\n",
       " 384,\n",
       " 5583,\n",
       " 588,\n",
       " 5206,\n",
       " 272,\n",
       " 88,\n",
       " 9458,\n",
       " 10,\n",
       " 377,\n",
       " 311,\n",
       " 3,\n",
       " 58,\n",
       " 454,\n",
       " 4408,\n",
       " 553,\n",
       " 1006,\n",
       " 6791,\n",
       " 821,\n",
       " 2139,\n",
       " 1219,\n",
       " 10,\n",
       " 588,\n",
       " 124,\n",
       " 340,\n",
       " 535,\n",
       " 2940,\n",
       " 26532,\n",
       " 9338,\n",
       " 1643,\n",
       " 1065,\n",
       " 3972,\n",
       " 851,\n",
       " 747]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = vocab(cleanedTrainData[0][0])\n",
    "indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<UNK>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleanedTrainData[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the longest document and padded every other document to reach max words\n",
    "longestDoc = max(cleanedTrainData, key= lambda x: len(x[0]))\n",
    "max_words = len(longestDoc[0])\n",
    "max_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45055, 376, 777, 648, 3, 98, 146, 6204, 9738, 2534, 5239, 70, 2905, 398, 819, 1618, 71, 70, 2534, 4358, 3676, 557, 1075, 3837, 187, 3573, 9682, 238, 5885, 2552, 2534, 28266, 2552, 2474, 5583, 10390, 2232, 13044, 3263, 345, 10775, 257, 355, 3774, 265, 256, 2534, 548, 124, 835, 18637, 304, 11, 182, 587, 17512, 3024, 5583, 384, 5583, 588, 5206, 272, 88, 9458, 10, 377, 311, 3, 58, 454, 4408, 553, 1006, 6791, 821, 2139, 1219, 10, 588, 124, 340, 535, 2940, 26532, 9338, 1643, 1065, 3972, 851, 747, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1456"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return a tensor for data and label\n",
    "normalizedData = []     # also turn each word into integer value\n",
    "target = []\n",
    "for text, label in cleanedTrainData:\n",
    "    normalizedEntry = vocab(text)\n",
    "    if len(text) < max_words:\n",
    "        normalizedEntry += [0] * (max_words - len(text))\n",
    "    normalizedData.append(normalizedEntry)\n",
    "    target.append(label)\n",
    "\n",
    "print(normalizedData[0])\n",
    "print(target[0])\n",
    "len(normalizedData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(target[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25000, 1456]) torch.Size([25000])\n"
     ]
    }
   ],
   "source": [
    "# Now turn them into tensor and we done\n",
    "X = torch.tensor(normalizedData, dtype=torch.int32)\n",
    "y = torch.tensor(target)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(45055, dtype=torch.int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test one word in the tensor\n",
    "X[0][0] # 45005 is the word 'If' in tokens -> GOOD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b83a821131512425ec2a991a5e6dc927ac0fbc9c159319a1177056e6b62cc881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
